NIST SP 800-188
September 2023



the results are reported. Differential privacy is based on information theory and makes no
distinction between which data are considered identifying and which are not. Nor does
using differential privacy require classifying data as direct identifers, quasi-identifers, or
non-identifying values. Instead, the most widely used variants of differential privacy –
such as ε-DP, (ε, δ )-DP, and zero-Concentrated Differential Privacy [23] – assume that all
values in a confdential record must be protected.
Differential privacy’s mathematical defnition is based on the intuition that the distinctive
attributes that contribute to an individuals’ data privacy cannot be violated by the analysis
of a dataset that does not contain that individual’s data. As a result, the mathematical
defnition of differential privacy is based on the idea of bounding the ratio of information
gain possible from an analysis of a dataset with and without the data of any possible single
individual. The defnition is usually satisfed by adding random noise to the result of a
query, ensuring that the added noise masks that hypothetical individual’s contribution. The
degree of sameness is defned by the parameter ε (epsilon) or ρ (rho). The smaller the
parameter ε or ρ, the more noise is added, and the more diffcult it is to distinguish the
contribution of a single individual. The result is increased privacy for all individuals –
both those in the sample and those in the population from which the sample is drawn
who are not present in the dataset. The research literature describes differential privacy
being used to solve a variety of tasks, including statistical analysis, machine learning, and
data sanitization [51]. However, the theory and practice of differential privacy is still in
their infancy, and at present, they are not suffciently well-developed enough to produce
privacy-protecting synthetic microdata that preserve interactions between more than a few
independent variables.13
Differential privacy can be implemented in an online query system or in a batch mode in
which an entire dataset is de-identifed at one time. In common usage, the phrase “differ-
ential privacy” is used to describe both the formal mathematical framework for evaluating
privacy loss and for algorithms that provably provide those formal privacy guarantees. Be-
cause of the potential for ambiguity, users of differential privacy should clearly state what
they mean by the term and what is actually differentially private in their system and data
release.
Algorithms that implement differential privacy do not guarantee that privacy will be pre-
served. Instead, these algorithms place mathematical bounds on what an attacker can learn
about the confdential information that was used to produce a specifc data publication. Ad-
ditionally, the privacy guarantee is limited to the privacy loss that an individual experiences
based on an analysis of their own data and not from the what the same individual might
experience from a statistical analysis of a dataset drawn from a population that does not
include the their personal data (see Sec. 3.2.1, “Probability of Re-Identifcation”).

13 For example, while the U.S. Census Bureau used differential privacy to produce the redistricting and demo-

 graphic data for the 2020 Census, it announced in December, 2022, that it was delaying the implementation
 of differential privacy for the American Community Survey [35].

                                                    16
