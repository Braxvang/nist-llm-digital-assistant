NIST SP 800-188
September 2023



As part of a risk analysis process, organizations should enumerate the specifc measures
that they will take to minimize the risk of successful re-identifcation. Organizations may
wish to consider both the actual risk and the perceived risk to those in the dataset and in the
broader community.
As part of the risk assessment, an organization may determine that there is no way to
achieve the de-identifcation goal in terms of data accuracy and identifability. In these
cases, the organization will need to decide whether it should adopt additional measures to
protect privacy (e.g., administrative controls or data use agreements), accept a higher level
of risk, or choose not to proceed with the project.
The “Privacy Risk Assessment” section of the NIST Privacy Engineering Program web-
site20 contains tools and use cases for agencies and others interested in conducting privacy
risk assessments.

3.2.3.     Impacts Other Than Re-Identifcation
The use of de-identifed data can lead to adverse impacts other than re-identifcation, though
risk assessments that evaluate the risks of re-identifcation can address these other risks as
well. Such risks might include:
    • The risk of excessive inferential disclosures
    • The risk that the de-identifcation process may introduce bias or inaccuracies into the
      dataset that result in incorrect decisions21
    • The risk that releasing a de-identifed dataset may reveal non-public information
      about an agency’s policies or practices
    • The risk that data may be overly generic or imprecise and of very little use to the
      intended recipients
It preferable to use de-identifcation processes that include assessments of data accuracy,
including bias and variability components (e.g., confdence intervals based on root mean
squared error). When it does not provide information that may aid data intruders, it is
also useful to reveal the de-identifcation process itself so that analysts can understand any
potential inaccuracies that might be introduced by the de-identifcation. This is consistent
with Kerckhoffs’s principle [86], a widely accepted system design principle that holds that
the security of a system should not rely on the secrecy of the methods that it employs.


20 The   Privacy Risk Assessment is available at https://www.nist.gov/itl/applied-cybersecurity/privacy-
   engineering/collaboration-space/focus-areas/risk-assessment.
21 For example, a personalized Warfarin dosing model created with data that had been modifed in a manner

   consistent with the differential privacy de-identifcation model produced higher mortality rates in simulation
   than a model created from unaltered data [62]. Educational data de-identifed with the k-anonymity model
   can also result in the introduction of bias that leads to spurious results [14, 155].


                                                      26
