NIST SP 800-188
September 2023



approach focuses on the process of de-identifcation rather than on the specifc properties
of the de-identifed data.
The Safe Harbor method of the HIPAA Privacy Rule [3] is an example of a prescriptive de-
identifcation standard. The intent of the Safe Harbor method is to “provide covered entities
with a simple method to determine if the information is adequately de-identifed” [118]. It
does this by specifying that health information is considered to be de-identifed through the
removal of 18 kinds of identifers and the explicit assurance that the entity does not have
actual knowledge that the remaining information can be used to identify an individual who
is the subject of the information. Once de-identifed, the dataset is no longer subject to
HIPAA privacy, security, and breach notifcation regulations. Nevertheless, “a covered en-
tity may require the recipient of de-identifed information to enter into a data use agreement
to access fles with known disclosure risk” [118].
As noted on page 5, Guidance from the U.S. Department of Health and Human Ser-
vices (HHS) on the Health Insurance Portability and Accountability Act (HIPAA) de-
identifcation standards states that even when properly applied, both the Safe Harbor and
expert determination methods for de-identifcation ”yield de-identifed data that retains
some risk of identifcation. Although the risk is very small, it is not zero, and there is
a possibility that de-identifed data could be linked back to the identity of the patient to
which it corresponds” [166]. Specifcally, the combination of quasi-identifers (indirect
identifers) left in the de-identifed dataset (e.g., race, gender, income and education level)
may raise re-identifcation issues.
The Privacy Rule states that a covered entity that employs the Safe Harbor method must
have no “actual knowledge” that the information – once de-identifed – could still be used
to re-identify individuals. However, covered entities are not obligated to employ experts
or mount re-identifcation attacks against datasets to verify that the use of the Safe Harbor
method has in fact resulted in data that cannot be re-identifed.
Prescriptive standards have the advantage of being relatively easy for users to follow, but
developing, testing, and validating such standards can be burdensome. Because prescriptive
de-identifcation standards do not depend on the particulars of a specifc case, there is
a tendency for them to be more conservative than is necessary, resulting in a decrease
in data for corresponding levels of risk. Even so, there is typically no mathematically
provable assurance that following a procedure specifed by the standard actually produces
the intended privacy-preserving outcome.
Agencies that create prescriptive de-identifcation standards should ensure that data de-
identifed according to the standards have a suffciently small risk of being re-identifed
consistent with the intended level of privacy protection. Such assurances frequently cannot
be made unless formal privacy techniques (e.g., differential privacy) are employed, and the
assurances made by differential privacy are unlike the simplistic absolute assurances made
in the context of traditional SDL techniques. Agencies may nevertheless determine that
public policy goals furthered by having an easy-to-use prescriptive standard outweighs the

                                             41
