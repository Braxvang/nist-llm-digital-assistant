NIST SP 800-188
September 2023



differential privacy can provide both security, privacy and even disassociability capabilities
(see Sec. 3.9.2).
A number of possible models exist at different points in the spectrum of security and pri-
vacy protections. Figure 4 summarizes this spectrum: its x-axis describes various privacy
techniques that can limit the informational content of the data; its y-axis describes how
much harm would occur if the underlying information were disclosed; and the regions of
the graph are labeled with suggested security techniques. Some common combinations of
security and privacy techniques include:
The release-and-forget model [123]. The de-identifed data may be released to the pub-
     lic, typically by being published on the internet. It can be diffcult or impossible for
     an organization to recall the data once released in this fashion and may limit infor-
     mation for future releases.
The Data Use Agreement (DUA) model. The de-identifed data may be made available
     under a legally binding data use agreement that details what can and cannot be done
     with the data. Typically, DUAs include security provisions and may prohibit at-
     tempted re-identifcation, linking to other data, and redistribution of the data without
     a similarly binding DUA. A DUA will typically be negotiated between the data holder
     and qualifed researchers (the “qualifed investigator model” [58]) or members of the
     general public (e.g., citizen scientists or the media), although they may be simply
     posted on the internet with a click-through license agreement that must be agreed to
     before the data can be downloaded (the “click-through model” [58]).
The synthetic data with validation model. Statistical disclosure limitation techniques are
     applied to the original dataset and used to create a synthetic dataset that contains
     many of the aspects of the original dataset but does not contain disclosing infor-
     mation. The synthetic dataset is released, either publicly or to vetted researchers.
     The synthetic dataset can then be used as a proxy for the original dataset, and if
     constructed well, the results of statistical analyses should be similar. If used in con-
     junction with the enclave model, researchers may use the synthetic dataset to develop
     queries and/or analytic software. These queries and/or software can then be taken to
     the enclave or provided to the agency and applied on the original data. There are a
     growing number of techniques for using differential privacy to create synthetic data.
The enclave model [58, 114, 141]. The de-identifed data may be kept in a segregated en-
     clave that restricts the export of the original data and instead accepts queries from
     qualifed researchers, runs the queries on the de-identifed data, and responds with
     results. Enclaves can be physical or virtual and can operate under a variety of dif-
     ferent models. For example, vetted researchers may travel to the enclave to perform
     their research, as is done with the Federal Statistical Research Data Centers operated
     by the U.S. Census Bureau. Enclaves may be used to implement the verifcation step
     of the synthetic data with validation model. Queries made in the enclave model may
     be vetted automatically or manually (e.g., by the DRB). Vetting can try to screen for

                                             33
