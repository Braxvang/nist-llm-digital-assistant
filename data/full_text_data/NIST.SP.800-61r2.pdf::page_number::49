                                                                               COMPUTER SECURITY INCIDENT HANDLING GUIDE



necessary data to meet their requirements. See Section 4 for additional information on sharing incident
data with other organizations.

Organizations should focus on collecting data that is actionable, rather than collecting data simply
because it is available. For example, counting the number of precursor port scans that occur each week
and producing a chart at the end of the year that shows port scans increased by eight percent is not very
helpful and may be quite time-consuming. Absolute numbers are not informative—understanding how
they represent threats to the business processes of the organization is what matters. Organizations should
decide what incident data to collect based on reporting requirements and on the expected return on
investment from the data (e.g., identifying a new threat and mitigating the related vulnerabilities before
they can be exploited.) Possible metrics for incident-related data include:

 Number of Incidents Handled.46 Handling more incidents is not necessarily better—for example,
  the number of incidents handled may decrease because of better network and host security controls,
  not because of negligence by the incident response team. The number of incidents handled is best
  taken as a measure of the relative amount of work that the incident response team had to perform, not
  as a measure of the quality of the team, unless it is considered in the context of other measures that
  collectively give an indication of work quality. It is more effective to produce separate incident
  counts for each incident category. Subcategories also can be used to provide more information. For
  example, a growing number of incidents performed by insiders could prompt stronger policy
  provisions concerning background investigations for personnel and misuse of computing resources
  and stronger security controls on internal networks (e.g., deploying intrusion detection software to
  more internal networks and hosts).
 Time Per Incident. For each incident, time can be measured in several ways:

     – Total amount of labor spent working on the incident
     – Elapsed time from the beginning of the incident to incident discovery, to the initial impact
         assessment, and to each stage of the incident handling process (e.g., containment, recovery)

     – How long it took the incident response team to respond to the initial report of the incident
     – How long it took to report the incident to management and, if necessary, appropriate external
         entities (e.g., US-CERT).
 Objective Assessment of Each Incident. The response to an incident that has been resolved can be
  analyzed to determine how effective it was. The following are examples of performing an objective
  assessment of an incident:

     – Reviewing logs, forms, reports, and other incident documentation for adherence to established
         incident response policies and procedures

     – Identifying which precursors and indicators of the incident were recorded to determine how
         effectively the incident was logged and identified

     – Determining if the incident caused damage before it was detected

46
     Metrics such as the number of incidents handled are generally not of value in a comparison of multiple organizations
     because each organization is likely to have defined key terms differently. For example, most organizations define “incident”
     in terms of their own policies and practices, and what one organization considers a single incident may be considered
     multiple incidents by others. More specific metrics, such as the number of port scans, are also of little value in
     organizational comparisons. For example, it is highly unlikely that different security systems, such as network intrusion
     detection sensors, would all use the same criteria in labeling activity as a port scan.


                                                               40
