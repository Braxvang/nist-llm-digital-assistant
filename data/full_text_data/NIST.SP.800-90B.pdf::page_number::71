                                                                                             NIST SP 800-90B                                         RECOMMENDATION FOR THE ENTROPY SOURCES
                                                                                                                                                             USED FOR RANDOM BIT GENERATION

                                                                                              Global performance        For a predictor, the number of accurate predictions over a long
                                                                                              metric                    period.

                                                                                                                        Testing within an implementation immediately prior to or during
                                                                                              Health testing            normal operation to determine that the implementation continues
                                                                                                                        to perform as implemented and as validated.

                                                                                                                        Two random variables X and Y are independent if they do not
                                                                                                                        convey information about each other. Receiving information
                                                                                              Independent               about X does not change the assessment of the probability
                                                                                                                        distribution of Y (and vice versa).
This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B




                                                                                              Independent and           A quality of a sequence of random variables for which each
                                                                                              Identically Distributed   element of the sequence has the same probability distribution as
                                                                                              (IID)                     the other values, and all values are mutually independent.

                                                                                                                        A dynamic-length data structure that stores a sequence of values,
                                                                                              List                      where each value is identified by its integer index.

                                                                                              Local performance         For a predictor, the length of the longest run of correct predictions
                                                                                              metric
                                                                                                                        A model for a probability distribution where the probability that
                                                                                                                        the ith element of a sequence has a given value depends only on
                                                                                              Markov model              the values of the previous n elements of the sequence. The model
                                                                                                                        is called an nth order Markov model.

                                                                                                                        The min-entropy (in bits) of a random variable X is the largest
                                                                                                                        value m having the property that each observation of X provides
                                                                                                                        at least m bits of information (i.e., the min-entropy of X is the
                                                                                                                        greatest lower bound for the information content of potential
                                                                                              Min-entropy               observations of X). The min-entropy of a random variable is a
                                                                                                                        lower bound on its entropy. The precise formulation for min-
                                                                                                                        entropy is (log2 max pi) for a discrete distribution having
                                                                                                                        probabilities p1, ...,pk. Min-entropy is often used as a worst-case
                                                                                                                        measure of the unpredictability of a random variable.

                                                                                                                        The maximum amount of information from the input that can
                                                                                                                        affect the output. For example, if f(x) = SHA-1(x) || 01, and x
                                                                                              Narrowest internal        consists of a string of 1000 binary bits, then the narrowest internal
                                                                                              width                     width of f(x) is 160 bits (the SHA-1 output length), and the output
                                                                                                                        width of f(x) is 162 bits (the 160 bits from the SHA-1 operation,
                                                                                                                        concatenated by 01).

                                                                                                                        The component of an entropy source that contains the non-
                                                                                              Noise source              deterministic, entropy-producing activity (e.g., thermal noise or
                                                                                                                        hard drive seek times).


                                                                                                                                             63
