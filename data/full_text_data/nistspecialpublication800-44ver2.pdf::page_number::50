                                                                    GUIDELINES ON SECURING PUBLIC WEB SERVERS



The following keywords are allowed:

 User-agent is the name of the robot or spider. A Web administrator may also include more than one
  agent name if the same exclusion is to apply to each specified bot. The entry is not case-sensitive (in
  other words, “googlebot” is the same as “GOOGLEBOT” and “GoogleBot”).

    An asterisk (“*”) indicates a “default” record, which applies if no other match is found. For example,
    if you specify “GoogleBot” only, then the “*” would apply to any other robot.

 Disallow tells the bot(s) specified in the user-agent field which sections of the Web site are excluded.
  For example, /images informs the bot not to open or index any files in the images directory or any
  subdirectories. Thus, the directory “/images/special/” would not be indexed by the excluded bot(s).

    Note that “/do” matches any directory beginning with “/do” (e.g. /do, /document, /docs, etc.), whereas
    “/do/” matches only the directory named “/do/”. A Web administrator can also specify individual
    files for exclusion. For example, the Web administrator could specify “/mydata/help.html” to prevent
    only that one file from being accessed by the bots. A value of just “/” indicates that nothing on the
    Web site is allowed to be accessed by the specified bot(s).

    At least one disallow per user-agent record must exist.

There are many ways to use the robots.txt file. Some simple examples are as follows:

 To disallow all (compliant) bots from specific directories:

        User-agent: *
        Disallow: /images/
        Disallow: /banners/
        Disallow: /Forms/
        Disallow: /Dictionary/
        Disallow: /_borders/
        Disallow: /_fpclass/
        Disallow: /_overlay/
        Disallow: /_private/
        Disallow: /_themes/

 To disallow all (compliant) bots from the entire Web site:

        User-agent: *
        Disallow: /

 To disallow a specific bot (in this case the Googlebot) from examining a specific Web page:

        User-agent: GoogleBot
        Disallow: tempindex.htm

Note that the robots.txt file is available to everyone and does not provide access control mechanisms to
the disallowed files. Thus, a Web administrator should not specify the names of sensitive files or folders
because attackers often analyze robots.txt files to guide their initial investigations of Web sites. If files or
directories must be excluded, it is better to use password-protected pages that cannot be accessed by bots.




                                                      5-8
