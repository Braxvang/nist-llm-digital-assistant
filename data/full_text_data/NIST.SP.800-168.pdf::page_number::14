3. Testing bytewise approximate matching algorithms

       The section gives a general overview of the motives and methods behind testing approximate
       matching algorithms. Defining a universal testing criteria is difficult; the effectiveness of a
       given approximate matching algorithm depends largely on the particular task against which it
       is evaluated. As such, rather than attempt to enumerate a comprehensive evaluation strategy,
       we focus on key considerations that arise during test design and evaluation. We also describe
       several properties of approximate matching algorithms that may be useful for characterizing
       their behavior.

 3.1   Motivation

       There are at least three major motivations for testing bytewise approximate matching
       algorithms. The first and most basic is to understand the classes of objects that the algorithm
       identifies as similar. For example, an algorithm may define its feature set such that it measures
       similarity as a function of the number of certain common byte sequences shared between two
       digital artifacts. Because this comparison occurs at a very low level, its semantic interpretation
       is not obvious. Testing across various tasks and types of data is necessary to determine whether
       such an algorithm is useful for identifying, say, html documents that contain similar content, or
       pdfs with the same embedded font.

       Second, as an algorithm is developed, a standard suite of tests, such as those proposed by
       Breitinger, Stivaktakis, and Baier [3], allows new versions to be compared against previous
       versions. Testing against a fixed set of tasks helps to highlight any changes in the algorithm’s
       similarity score as a function of its input. Improvements in speed or space efficiency can also
       be measured.

       Finally, testing allows approximate matching algorithms to be compared to each other. The use
       of a standard suite of tasks can be beneficial in this testing scenario also. Additional measures
       must be taken, however, to ensure that tests produce comparable results. Because the
       algorithms define their similarity score independently, direct comparison of scores is rarely
       meaningful. One solution for this problem is to use a threshold or some other criteria to
       translate scores from their [0,1] range back to a binary {0,1} output which can be evaluated
       against known ground truth to produce confusion matrices (Roussev’s comparison of sdhash
       and ssdeep offers an example of this approach [2]). Alternately, the tester might define other
       high-level criteria for the expected behavior of an algorithm in a specific use case of interest,
       and evaluate the algorithms against this expectation.

 3.2   Test Data

       Approximate matching algorithms can be tested against either controlled (synthetic) data [7] or
       real data (i.e., data originally created for purposes other than testing). The former typically
       consists of randomly generated blocks or files; its main advantage is that ground truth is
       constructed and, therefore, precisely known. This allows tests to be run automatically and the
       results to be interpreted with standard statistical measures.

       The drawback to using randomly generated data is that it is not necessarily representative. For
       example, random data tends to be unique, high-entropy and internally varied. These properties
       may be appropriate for use cases involving compressed or encrypted data, but could prove

                                                  7
