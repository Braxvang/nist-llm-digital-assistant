NIST SP 800-188
September 2023



50]. Abowd and Hawes show how these privacy-preserving defnitions relate to identity
and attribute disclosures [7, p. 12].
According to Kifer et al., the “confdentiality-breaching versions of identity, attribute and
inferential disclosure all have the same mathematical formulation” [88, p. 4]. Careful def-
initions show that identity and attribute disclosure are merely high-confdence cases of
inferential disclosure. Additional information about disclosure can be found in Sec. 3.1.
Disclosure limitation is a general term for the practice of allowing summary information
or queries on data within a dataset to be released without revealing information about spe-
cifc individuals whose personal information is contained within the dataset. Thus, de-
identifcation is a kind of disclosure limitation technique. Every disclosure limitation pro-
cess introduces inaccuracy into the results [14, 11].
A primary goal of disclosure limitation is to protect the privacy of individuals while avoid-
ing the introduction of non-ignorable biases [8] (e.g., bias that might lead a social scientist
to come to the wrong conclusion) into the de-identifed dataset. One way to measure the
amount of bias that has been introduced by the de-identifcation process is to compare
statistics or models generated by analyzing the original dataset with those that are gener-
ated by analyzing the de-identifed datasets. Such biases introduced by the de-identifcation
process are typically unrelated to any statistical biases that may also exist in the original
data.
Formal models of privacy can quantify the privacy protection offered by a de-identifcation
process. With methods based on differential privacy, this measurement takes the form of
a number called privacy loss, which quantifes the additional risk that an adversary might
learn something new about an individual as the result of a de-identifed data release. When
a de-identifcation process is associated with low privacy loss, releasing the data it produces
should result in only a small amount of additional risk for individuals in the input dataset.
Some formal models, such as differential privacy, allow for composing the privacy losses
of multiple data releases to quantify the increased privacy loss to individuals from each
release, even if an attacker attempts to combine the releases in unanticipated ways. Other
formal models, such as k-anonymity, do not have this capability.
An upper bound on the total acceptable privacy loss of many data releases is often called
a privacy loss budget or simply a privacy budget. This number quantifes the total privacy
loss to an individual who participates in all of the releases.
Differential privacy [53] is a rigorous mathematical defnition of disclosure that consid-
ers the increase in accuracy with which an individual’s confdential data may be estimated
as a result of a mathematical analysis based on that data being made publicly available.
Statisticians, mathematicians, and other kinds of privacy engineers then develop mathe-
matical algorithms called mechanisms, which process data in a way that is consistent with
the defnition. Differential privacy limits properly defne inferential disclosure by adding
non-deterministic noise (random values) to the results of mathematical operations before


                                              15
