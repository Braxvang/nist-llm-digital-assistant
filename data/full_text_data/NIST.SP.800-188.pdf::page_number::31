NIST SP 800-188
September 2023



K-anonymity [137, 152] is a framework for quantifying the diffculty of singling out indi-
vidual data records based on attributes termed quasi-identifers14 that are potentially iden-
tifying. The technique is based on the concept of an equivalence class – the set of records
that have the same values on the quasi-identifers. A dataset is said to be k-anonymous
if there are no fewer than k matching records for every specifc combination of quasi-
identifers. For example, if a dataset that has the quasi-identifers (birth year) and (state)
has k = 4 anonymity, then there must be at least four records for every combination of (birth
year, state). Subsequent work has refned k-anonymity by adding requirements for the di-
versity of the sensitive attributes within each equivalence class (known as l-diversity [98])
and requiring that the resulting data be statistically close to the original data (known as
t-closeness [94]).
Dozens of other privacy metrics beyond differential privacy and k-anonymity have been
proposed. See “Technical Privacy Metrics: A Systematic Survey” for a more comprehen-
sive listing [169].
All privacy techniques require the use of subject-matter experts (SMEs) to design the spe-
cifc application of the technique and to validate that it has been implemented properly.
For example, differential privacy requires experts to validate that all uses of confdential
data go through the differential privacy mechanism, while k-anonymity requires the use of
experts to determine the set of quasi-identifers by distinguishing between identifying and
non-identifying information.
The k-anonymity technique and its subsequent refnements defne formal privacy models
but come with an important drawback: k-anonymity and related techniques are not compo-
sitional. That is, they do not quantify the cumulative privacy loss of multiple data releases,
and multiple releases can interact in unpredictable ways, resulting in a catastrophic loss of
privacy.
Finally, confdentiality breaches are not the only kinds of risks that can result from the re-
lease of de-identifed data. Analysts working with de-identifed data often have no way of
knowing if the de-identifcation process resulted in the addition of signifcant noise or bias
in the resulting data. Thus, de-identifcation operations intended to protect the confdential-
ity of data could result in inaccurate research fndings. For agencies with research missions,
this threatens the integrity and reputation of the agency and calls into question the ability
of the agency to fulfll its mission objectives. Such research might also cause harm if it
is used to support harmful policies. Thus, agencies that release de-identifed data should
explore options for describing the statistical impact that the de-identifcation process has
on the data.




14 A quasi-identifer, also known as an indirect identifer, is a variable that can be used to identify an individual

  through association with other information. For other discussions regarding “singling out,” see [33].


                                                       17
