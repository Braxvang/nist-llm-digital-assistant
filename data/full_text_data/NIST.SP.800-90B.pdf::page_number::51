                                                                                             NIST SP 800-90B                                                  RECOMMENDATION FOR THE ENTROPY SOURCES
                                                                                                                                                                      USED FOR RANDOM BIT GENERATION

                                                                                                                                                                                âˆ
                                                                                                      and Î“(a,b) is the incomplete Gamma function defined as âˆ«ğ‘ğ‘ ğ‘¡ğ‘¡ ğ‘ğ‘âˆ’1 ğ‘’ğ‘’ âˆ’ğ‘¡ğ‘¡ ğ‘‘ğ‘‘ğ‘‘ğ‘‘. An efficient
                                                                                                      implementation of F(1/z) is provided in Appendix G.1.1. The bounds of the binary search
                                                                                                      should be 1/2 and 1.

                                                                                                  8. If the binary search yields a solution, then the min-entropy estimation is the negative
                                                                                                     logarithm of the parameter, p:
                                                                                                                                        min-entropy = â€“log2( p).

                                                                                                      If the search does not yield a solution, then the min-entropy estimation is:

                                                                                                                                       min-entropy = log2(2)=1.
This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B




                                                                                             Example: Suppose that S = (1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
                                                                                             0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0). The collisions of the sequence are (1, 0, 0), (0, 1, 1), (1, 0, 0), (1, 0,
                                                                                             1), (0, 1, 0), (1, 1), (1, 0, 0), (1, 1), (0, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (0, 1, 0), (1, 1). After step 5,
                                                                                             v=14, and the sequence (t1, â€¦ tv) is (3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2). Then ğ‘‹ğ‘‹ï¿½ = 2.7143, ğœğœï¿½ =
                                                                                             0.4688, and ğ‘‹ğ‘‹ï¿½ â€² = 2.3915. The solution to the equation is p = 0.7329, giving an estimated min-
                                                                                             entropy of 0.4483.

                                                                                             6.3.3    The Markov Estimate

                                                                                             In a first-order Markov process, the next sample value depends only on the latest observed sample
                                                                                             value; in an nth-order Markov process, the next sample value depends only on the previous n
                                                                                             observed values. Therefore, a Markov model can be used as a template for testing sources with
                                                                                             dependencies. The Markov estimate provides a min-entropy estimate by measuring the
                                                                                             dependencies between consecutive values from the input dataset. The min-entropy estimate is
                                                                                             based on the entropy present in any subsequence (i.e., chain) of outputs, instead of an estimate of
                                                                                             the min-entropy per output.

                                                                                             Samples are collected from the noise source, and specified as d-long chains of samples. From this
                                                                                             data, probabilities are determined for both the initial state and transitions between any two states.
                                                                                             These probabilities are used to determine the highest probability of any particular d-long chain of
                                                                                             samples. The corresponding maximum probability is used to determine the min-entropy present in
                                                                                             all such chains generated by the noise source. This min-entropy value is particular to d-long chains
                                                                                             and cannot be extrapolated linearly; i.e., chains of length wd will not necessarily have w times as
                                                                                             much min-entropy present as a d-long chain. It may not be possible to know what a typical output
                                                                                             length will be at the time of testing. Therefore, although not mathematically correct, in practice,
                                                                                             calculating an entropy estimate per sample (extrapolated from that of the d-long chain) provides
                                                                                             estimates that are close.

                                                                                             This entropy estimation method is only applied to binary inputs.

                                                                                             Given the input S = (s1, â€¦, sL), where si Ïµ A = {0,1},
                                                                                                                                                                           #{0 in ğ‘†ğ‘†}
                                                                                             1.      Estimate the initial probabilities for each output value, ğ‘ƒğ‘ƒ0 =                    and ğ‘ƒğ‘ƒ1 = 1 âˆ’ ğ‘ƒğ‘ƒ0 .
                                                                                                                                                                               ğ¿ğ¿
                                                                                             2.      Let T be the 2Ã—2 transition matrix of the form

                                                                                                                                                     43
