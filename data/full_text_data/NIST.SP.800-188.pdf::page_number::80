NIST SP 800-188
September 2023



    • Fully synthetic datasets do not have a zero-disclosure risk because they still contain
      information derived from non-public information about individuals. The disclosure
      risk may be greater when synthetic data are created with traditional statistical mod-
      eling or data imputing techniques rather than formal privacy models, such as differ-
      ential privacy, which have provisions for tracking the accumulated privacy loss that
      results from multiple data operations, as discussed in Sec. 4.4.7.

4.4.7.   Creating a Synthetic Dataset with Diferential Privacy
A growing number of mathematical algorithms have been developed for creating syn-
thetic datasets that meet the mathematical defnition of privacy provided by differential
privacy [53]. Most of these algorithms will transform a dataset that contains private data
into a new dataset containing synthetic data that provides reasonably accurate results in re-
sponse to a variety of queries. However, there is no algorithm or implementation currently
in existence that can be used by a person who is unskilled in the area of differential privacy.
This is an area of active research.
The idea of differential privacy is that the result of a data analysis function κ applied to
a dataset should not change very much if an arbitrary person p’s data are added to or
removed from a dataset D. That is, κ(D) ≈ κ(D − p). The degree to which the two values
are approximately equal is determined by the privacy loss parameter ε.
In the mathematical formulation of pure differential privacy, ε can range from 0 to ∞. When
ε = 0, the output of κ does not depend on the input dataset. When ε = ∞, the output of κ is
entirely dependent upon the input dataset, such that changing a single record results in an
unambiguous measurable change in κ’s output. Thus, larger values of ε provide for more
accuracy but result in increased privacy loss.
When ε is set appropriately, differential privacy limits the privacy loss that a data subject
experiences from the use of their private data to the maximum privacy loss necessary for
a given statistical purpose. Note that this particular notion of privacy does not protect
all secrets about a person. It only protects the secrets that an observer would not have
been able to learn if the person’s data were not present in the dataset. Stated another
way, differential privacy protects individuals from additional harm that may result from
their participation in the data analysis but does not protect them from harm that would
have occurred even if their data were not present. For example, if a study concludes that
residents of Vermont overwhelmingly drive 4-wheel-drive vehicles, one might conclude
that a particular Vermonter drives a 4-wheel-drive vehicle even if that individual did not
participate in the study. Differential privacy does not attempt to prevent inferences of this
type.
Many academic papers on differential privacy assume a value of 1.0 for ε but do not ex-
plain the rationale of the choice. Some researchers who work in the feld of differential
privacy have tried mapping existing privacy regulations to the choice of ε, but these efforts


                                              66
