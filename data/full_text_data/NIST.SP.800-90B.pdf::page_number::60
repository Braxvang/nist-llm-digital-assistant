                                                                                             NIST SP 800-90B                                                         RECOMMENDATION FOR THE ENTROPY SOURCES
                                                                                                                                                                             USED FOR RANDOM BIT GENERATION

                                                                                                     where

                                                                                                                                                     ğ‘ğ‘ = 1 âˆ’ ğ‘ƒğ‘ƒğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™

                                                                                                     and x = x10, derived by iterating the recurrence relation
                                                                                                                                                                ğ‘Ÿğ‘Ÿ           ğ‘Ÿğ‘Ÿ+1
                                                                                                                                                ğ‘¥ğ‘¥ğ‘—ğ‘— = 1 + ğ‘ğ‘ğ‘ƒğ‘ƒğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ ğ‘¥ğ‘¥ğ‘—ğ‘—âˆ’1

                                                                                                     for j from 1 to 10, and x0=1. Note that solving for ğ‘ƒğ‘ƒğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ using the logarithm of these
                                                                                                     equations is robust against overflows. Table 3 in Appendix G.2 provides some pre-
                                                                                                     calculated values of ğ‘ƒğ‘ƒğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ .
This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B




                                                                                                 7. The min-entropy is the negative logarithm of the greater performance metric
                                                                                                                                                          â€²                               1
                                                                                                                             min-entropy = âˆ’log 2 (max(ğ‘ƒğ‘ƒğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘” , ğ‘ƒğ‘ƒğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ , ğ‘˜ğ‘˜)).

                                                                                             Example: Suppose that S = (2, 1, 3, 2, 1, 3, 1, 3, 1, 2), so that L = 10 and N = 9. For the purpose of
                                                                                             this example, suppose that D = 3 (instead of 128). The following table shows the values in step 3.

                                                                                                          i       lag           Winner          prediction          si       correcti-1       scoreboard
                                                                                                                               (step 3b)                                                       (step 3d)
                                                                                                          2    (2, --, --)         1                  2             1               0           (0, 0, 0)
                                                                                                          3    (1, 2, --)          1                  1             3               0           (0, 0, 0)
                                                                                                          4    (3, 1, 2)           1                  3             2               0           (0, 0, 1)
                                                                                                          5    (2, 3, 1)           3                  1             1               1           (0, 0, 2)
                                                                                                          6    (1, 2, 3)           3                  3             3               1           (0, 0, 3)
                                                                                                          7    (3, 1, 2)           3                  2             1               0           (0, 1, 3)
                                                                                                          8    (1, 3, 1)           3                  1             3               0           (0, 2, 3)
                                                                                                          9    (3, 1, 3)           3                  3             1               0           (0, 3, 3)
                                                                                                         10    (1, 3, 1)           2                  3             2               0           (0, 3, 3)


                                                                                             After all of the predictions are made, correct = (0, 0, 0, 1, 1, 0, 0, 0, 0). Then, ğ‘ƒğ‘ƒğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘” = 0.2222,
                                                                                                â€²
                                                                                             ğ‘ƒğ‘ƒğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘” = 0.6008, ğ‘ƒğ‘ƒğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ = 0.1167, and the resulting min-entropy estimate is 0.735.

                                                                                             6.3.9   The MultiMMC Prediction Estimate

                                                                                             The MultiMMC predictor is composed of multiple Markov Model with Counting (MMC)
                                                                                             subpredictors. Each MMC predictor records the observed frequencies for transitions from one
                                                                                             output to a subsequent output (rather than the probability of a transition, as in a typical Markov
                                                                                             model), and makes a prediction, based on the most frequently observed transition from the current
                                                                                             output. MultiMMC contains D MMC subpredictors running in parallel, one for each depth from 1
                                                                                             to D. For example, the MMC with depth 1 creates a first-order model, while the MMC with depth
                                                                                             D creates a Dth-order model. MultiMMC keeps a scoreboard that records the number of times that
                                                                                             each MMC subpredictor was correct, and uses the subpredictor with the most correct predictions
                                                                                             to predict the next value.


                                                                                                                                                          52
