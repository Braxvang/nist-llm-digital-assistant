Artificial Intelligence (AI)
NIST contributes to the research, standards, evaluation, and data required to advance the
development, use, and governance of trustworthy AI. NIST aims to cultivate trust in the design,
development, and use of AI technologies and systems by improving measurement science,
technology, standards, and related tools in ways that enhance economic security and improve
quality of life.
AI and Machine Learning (ML) are already changing the ways in which society addresses economic
and national security challenges and opportunities, and these technologies must be developed and
used in a trustworthy and responsible manner. Characteristics that support trustworthiness include
accuracy, explainability, interpretability, reliability, privacy, robustness, safety, security (resilience),
and the mitigation of harmful bias. Principles such as transparency, fairness, and accountability
should be considered, especially during deployment and use. Many of these characteristics and
principles are described in the work and publications at NIST ITL’s Artificial Intelligence site.
Trustworthy data, standards, evaluation, validation, and verification are critical for the successful
deployment of new technologies for genomics, image and video processing, materials, natural
language processing, robotics, wireless spectrum monitoring, and more.
Delivering the needed measurements, standards, and other tools is a primary focus of NIST’s
portfolio of AI efforts. It is an area in which NIST has special responsibilities and expertise and
for which others often turn to NIST for guidance. NIST's AI goals and activities are prioritized and
informed by its statutory mandates, White House directives, and the needs expressed by industry,
other federal agencies, and the global AI research community.
NIST’s continued AI work is aligned with five broad goals:
1. Conduct fundamental research to advance trustworthy AI technologies.
2. Apply AI research and innovation across the NIST Laboratory Programs.
3. Establish benchmarks and develop data and metrics to evaluate AI technologies.
4. Lead and participate in the development of technical AI standards.
5. Contribute to discussions and the development of AI policies.
The NCCoE, in collaboration with industry and academic partners, has developed Dioptra, an
experimentation testbed to address the broader challenge of the evaluation of ML algorithms'
defenses and their resistance to attack. The testbed aims to facilitate security evaluations of ML
algorithms under a diverse set of conditions. The testbed has a modular design that enables
researchers to easily substitute alternative datasets, models, attacks, and defenses. The result
is the ability to advance the metrology needed to help secure ML-enabled systems. NIST is
expanding the user base for Dioptra and developing additional capabilities.
NIST has also initiated projects to address cybersecurity challenges in AI-enabled healthcare and
semi-autonomous vehicles. The outcome of these efforts will be to provide practical guidance
on addressing these challenges. NIST will continue to collaborate with commercial, academic,
and public-sector partners to pursue these goals and collectively advance the security and
trustworthiness of this important and emerging technology.



NIST/ITL FY 2021 ANNUAL CYBERSECURITY AND PRIVACY REPORT
6 | Risk Management                                                                                            24
