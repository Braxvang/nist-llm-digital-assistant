                                                                                             NIST SP 800-90B                                                           RECOMMENDATION FOR THE ENTROPY SOURCES
                                                                                                                                                                               USED FOR RANDOM BIT GENERATION

                                                                                                                                                             1
                                                                                                                                                                 âˆ’ğ‘˜ğ‘˜
                                                                                                                                               ğ‘§ğ‘§ +               1
                                                                                                                                                       1+          1âˆ’ğ‘˜ğ‘˜
                                                                                                                                                            ğ‘§ğ‘§+       2
                                                                                                                                                                1+
                                                                                                                                                                      2âˆ’ğ‘˜ğ‘˜
                                                                                                                                                                   ğ‘§ğ‘§+ 3
                                                                                                                                                                      1+
                                                                                                                                                                        â€¦

                                                                                             G.2     Predictors

                                                                                             Shannon first published the relationship between the entropy and predictability of a sequence in
                                                                                             1951 [Shan51]. Predictors construct models from previous observations, which are used to predict
This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B




                                                                                             the next value in a sequence. The prediction-based estimation methods in this Recommendation
                                                                                             work in a similar way, but attempt to find bounds on the min-entropy of integer sequences
                                                                                             generated by an unknown process (rather than the N-gram entropy of English text, as in [Shan51]).

                                                                                             The predictor approach uses two metrics to produce an estimate. The first metric is based on the
                                                                                             global performance of the predictor, called accuracy in machine-learning literature. Essentially, a
                                                                                             predictor captures the proportion of guesses that were correct. This approximates how well one
                                                                                             can expect a predictor to guess the next output from a noise source, based on the results over a
                                                                                             long sequence of guesses. The second metric is based on the greatest number of correct predictions
                                                                                             in a row, which is called the local performance metric. This metric is useful for detecting cases
                                                                                             where a noise source falls into a highly predictable state for some time, but the predictor may not
                                                                                             perform well on long sequences. The calculations for the local entropy estimate come from the
                                                                                             probability theory of runs and recurrent events [Fel50]. For more information about min-entropy
                                                                                             estimation using predictors, see [Kel15].

                                                                                             In order to make the predictor estimates lean toward a conservative underestimate of min-entropy,
                                                                                                                              â€²
                                                                                             ğ‘ƒğ‘ƒğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘” is replaced by ğ‘ƒğ‘ƒğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘” , the proportion corresponding to the 99th percentile of the number of
                                                                                             correct predictions based on the observed number of correct predictions. Note that the order in
                                                                                             which correct predictions occur does not influence the min-entropy estimate based on ğ‘ƒğ‘ƒğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘” . For
                                                                                             example, a predictor could always be correct for the first half of the outputs in a data set, and
                                                                                             always incorrect for the second half of the outputs. The min-entropy estimate of this sequence,
                                                                                             based on ğ‘ƒğ‘ƒğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘” , is half the data length in bits. On the other hand, for another sequence, the
                                                                                             predictor could have a 50 % chance of being correct for every sample in this sequence. The min-
                                                                                             entropy estimate of this second sequence, based on ğ‘ƒğ‘ƒğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘” , is the same as that of the first sequence.
                                                                                             However, the typical successful prediction run lengths are very different for these two sequences.
                                                                                             Therefore, the approach takes the local prediction performance into account in order to
                                                                                             conservatively decrease the min-entropy estimate if the observed local prediction behavior is
                                                                                             statistically significant, given the global prediction success rate. The predictor estimates
                                                                                                                                                                              â€²
                                                                                             accomplish this by basing the min-entropy estimate on max(ğ‘ƒğ‘ƒğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘              , ğ‘ƒğ‘ƒğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ ), where ğ‘ƒğ‘ƒğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ is the
                                                                                             successful prediction proportion for which the observed longest run of correct predictions is the
                                                                                                                                                                                          â€²
                                                                                             99th percentile. This is effectively a one-tail hypothesis test that rejects ğ‘ƒğ‘ƒğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”             in favor of ğ‘ƒğ‘ƒğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ if
                                                                                                                                                                       â€²
                                                                                             the observed longest run, given a success probability of ğ‘ƒğ‘ƒğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘” , is beyond the 99th percentile.




                                                                                                                                                            75
