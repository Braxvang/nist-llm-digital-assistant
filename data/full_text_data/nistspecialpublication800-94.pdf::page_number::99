                                                   GUIDE TO INTRUSION DETECTION AND PREVENTION SYSTEMS (IDPS)



 Many IDPS components are appliance-based and have many hardware models and configurations
  available, each with its own performance characteristics. Other IDPS components are not appliance-
  based, so their hardware, OSs, and OS configurations may vary widely, which can all affect
  performance.
 There are no open standards for performance testing, nor are there publicly available, comprehensive,
  up-to-date test suites.
Accordingly, evaluators should focus on the general performance characteristics of IDPS products and
avoid differentiating products by slight differences in reported performance capabilities. Vendors
typically rate their products by maximum capacity, such as the volume of network traffic or number of
packets per second monitored for network-based IDPS, the number of events monitored per second for
host-based IDPS, or the flows monitored per second or the number of hosts that can be profiled for NBA
systems. Section 9.6 provides guidance on collecting data on IDPS performance as part of an evaluation.
When evaluating maximum capacity claims, evaluators should consider the following questions:

 Does the maximum capacity reflect activity that is being analyzed or activity that is being monitored
  but not necessarily analyzed? For example, a network-based IDPS might perform little or no analysis
  on the use of certain application protocols.
 What was the nature of the activity used to measure capacity? This knowledge can help evaluators to
  determine if the testing used an environment similar to their own or had significant differences that
  could affect performance results. Aspects of this to consider include the following:

    –   How was the activity used for testing generated?

    –   What types of malicious activity were included in the testing? What percentage of the events
        monitored by the IDPS was malicious? What percentage of the malicious events was detected by
        the IDPS under maximum load?

    –   For network traffic, what protocols were used and in roughly what percentages? For host-based
        activity, what applications were run, and what other sources of events were used?

    –   How closely did the activity used for testing reflect the actual conditions of the production
        environment?
 How was the IDPS configured? Was the default configuration used? If not, what detection
  capabilities, logging capabilities, and other features were enabled or disabled from the default?
 For any non-appliance components, what hardware, OSs, and applications or services were in use?
 Who performed the testing?
 When was the testing performed?
Evaluators should also consider the performance features that each IDPS under consideration offers.
Possible considerations for performance features include the following:

 Does the IDPS offer any performance tuning features, either manually configured or automatically
  implemented? For example, if an IDPS is being overwhelmed by high volumes of activity, can it
  alter its detection capabilities so that it temporarily performs less extensive analysis on all the traffic
  or stops analyzing low-risk traffic?




                                                     9-7
