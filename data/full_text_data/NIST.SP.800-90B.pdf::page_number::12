                                                                                             NIST SP 800-90B                                                  RECOMMENDATION FOR THE ENTROPY SOURCES
                                                                                                                                                                      USED FOR RANDOM BIT GENERATION

                                                                                             2     General Discussion

                                                                                             The three main components of a cryptographic RBG are a source of random bits (an entropy
                                                                                             source), an algorithm for accumulating and providing random bits to the consuming applications,
                                                                                             and a way to combine the first two components appropriately for cryptographic applications. This
                                                                                             Recommendation describes how to design and test entropy sources. SP 800-90A describes
                                                                                             deterministic algorithms that take an entropy input and use it to produce pseudorandom values. SP
                                                                                             800-90C provides the ‚Äúglue‚Äù for putting the entropy source together with the algorithm to
                                                                                             implement an RBG.

                                                                                             Specifying an entropy source is a complicated matter. This is partly due to confusion in the
This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B




                                                                                             meaning of entropy, and partly due to the fact that, while other parts of an RBG design are strictly
                                                                                             algorithmic, entropy sources depend on physical processes that may vary from one instance of a
                                                                                             source to another. This section discusses, in detail, both the entropy source model and the meaning
                                                                                             of entropy.

                                                                                             2.1   Min-Entropy

                                                                                             The central mathematical concept underlying this Recommendation is entropy. Entropy is defined
                                                                                             relative to one‚Äôs knowledge of an experiment‚Äôs output prior to observation, and reflects the
                                                                                             uncertainty associated with predicting its value ‚Äì the larger the amount of entropy, the greater the
                                                                                             uncertainty in predicting the value of an observation. There are many possible measures for
                                                                                             entropy; this Recommendation uses a very conservative measure known as min-entropy, which
                                                                                             measures the effectiveness of the strategy of guessing the most likely output of the entropy source.
                                                                                             (see Appendix D and [Cac97] for more information).

                                                                                             In cryptography, the unpredictability of secret values (such as cryptographic keys) is essential. The
                                                                                             probability that a secret is guessed correctly in the first trial is related to the min-entropy of the
                                                                                             distribution that the secret was generated from.

                                                                                             The min-entropy of an independent discrete random variable X that takes values from the set
                                                                                             A={x1,x2,‚Ä¶,xk} with probability Pr(X=xi) = pi for i =1,‚Ä¶,k is defined as

                                                                                                                                   ùêªùêª = min (‚àílog 2 ùëùùëùùëñùëñ ),
                                                                                                                                          1‚â§ùëñùëñ‚â§ùëòùëò

                                                                                                                                      = ‚àí log 2 max ùëùùëùùëñùëñ .
                                                                                                                                                    1‚â§ùëñùëñ‚â§ùëòùëò

                                                                                             If X has min-entropy H, then the probability of observing any particular value for X is no greater
                                                                                             than 2 ‚àíH. The maximum possible value for the min-entropy of a random variable with k distinct
                                                                                             values is log2 k, which is attained when the random variable has a uniform probability distribution,
                                                                                             i.e., p1 = p2 =‚Ä¶= pk =1/k.




                                                                                                                                                4
