                                                                                             NIST SP 800-90B                                                        RECOMMENDATION FOR THE ENTROPY SOURCES
                                                                                                                                                                            USED FOR RANDOM BIT GENERATION

                                                                                             This sanity check is based on a binomial test, where there are two possible outcomes for each trial:
                                                                                             the most frequent value or any other value is observed. The purpose of the test is to determine
                                                                                             whether the most frequent value appears more than would be expected, given the initial entropy
                                                                                             estimate, HI. The probability of type I error, denoted Î±, is set at 0.01 over the entire sanity check,
                                                                                             where each of the 2000 binomial experiments 3 has type I error probability of 0.000 005.

                                                                                             Only the experiment yielding the highest count is tested. If that experiment passes the test, then
                                                                                             the other 1999 experiments will pass as well. If any of the 2000 experiments were to fail, one of
                                                                                             the failed experiments would be the experiment having the highest count. Therefore, it is sufficient
                                                                                             to test the experiment with the highest count.

                                                                                             Given the 1000 by 1000 restart matrix and the initial entropy estimate HI, the test is performed as
This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800-90B




                                                                                             follows:

                                                                                             1. Let p = 2âˆ’ğ»ğ»ğ¼ğ¼ . Let Î± be 0.000 005.
                                                                                             2. For each row (1â‰¤ i â‰¤ 1000) of the matrix, count the number of occurrences of each sample
                                                                                                present in the row. Set XRi to the highest count value for row i. Let XR be the maximum count
                                                                                                value for all the rows, i.e., XR = max (XR1,â€¦, XR1000).
                                                                                             3. For each column (1â‰¤ i â‰¤ 1000) of the matrix, count the number of occurrences of each sample
                                                                                                present in the column. Set XCi to the highest count value for column i. Let XC be the maximum
                                                                                                count value for all the columns, i.e., XC = max (XC1,â€¦, XC1000).
                                                                                             4. Let ğ‘‹ğ‘‹ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š = max(ğ‘‹ğ‘‹ğ‘ğ‘ , ğ‘‹ğ‘‹ğ‘…ğ‘… ).
                                                                                                                                            1000
                                                                                             5. Calculate P(ğ‘‹ğ‘‹ â‰¥ ğ‘‹ğ‘‹ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š ) = âˆ‘1000                 ğ‘—ğ‘—
                                                                                                                               ğ‘—ğ‘—=ğ‘‹ğ‘‹ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š ï¿½ ğ‘—ğ‘— ï¿½ ğ‘ğ‘ (1 âˆ’ ğ‘ğ‘)
                                                                                                                                                             1000âˆ’ğ‘—ğ‘—
                                                                                                                                                                     . If Pr(ğ‘‹ğ‘‹ â‰¥ ğ‘‹ğ‘‹ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š ) < ğ›¼ğ›¼, the test
                                                                                                fails. Otherwise, the test passes.
                                                                                             3.1.5    Entropy Estimation for Entropy Sources Using a Conditioning Component

                                                                                             The optional conditioning component gets inputs from the noise source and generates the output
                                                                                             of the entropy source. The size of the input and the output of the conditioning component in bits,
                                                                                             denoted as nin and nout, respectively, shall be fixed and shall be specified by the submitter. Noise
                                                                                             source outputs are concatenated to construct nin-bit input to the conditioning function. The entropy
                                                                                             of the input, denoted hin, depends on the number of samples needed to construct the nin-bit input.
                                                                                             If w samples are needed, then hin is estimated to be wÃ—h bits. The size of the conditioning
                                                                                             component input shall be a multiple of the size of the noise source output.

                                                                                             Since the conditioning component is deterministic, the entropy of the output is at most hin.
                                                                                             However, the conditioning component may reduce the entropy of the output. The entropy of the
                                                                                             output from the conditioning component is denoted as hout, i.e., hout bits of entropy are contained
                                                                                             within the nout-bit output. The entropy of the output also depends on the internals of the
                                                                                             conditioning components. In this Recommendation, the narrowest internal width within the




                                                                                             3 The experiments done for each row or column are considered to be independent.




                                                                                                                                                          14
