NIST SP 800-188
September 2023



       control (SDC) [61, 38, 47].6 Statistical agencies created these methods so that they
       could release statistical tables and public use fles (PUF) to allow users to learn infor-
       mation and perform original research while protecting the privacy of the individuals
       in the dataset. SDL is widely used in contemporary statistical reporting.
   2. In the 1990s, there was a signifcant increase in the release of microdata fles for
      public use in the form of both individual responses from surveys and administrative
      records. Initially, these releases merely stripped obviously identifable information,
      such as names and Social Security numbers (what are now called direct identifers).
      Following some releases, researchers discovered that it was possible to re-identify
      individuals’ data by triangulating some of the remaining data (now called quasi-
      identifers or indirect identifers [37]). The research resulted in the creation of the
      k-anonymity model for protecting privacy [153, 137, 138, 152],7 which is refected
      in the Offce of Civil Rights guidance on how to apply de-identifcation in a manner
      consistent with the HIPAA Privacy Rule [118]. Today, variants of k-anonymity are
      commonly used for sharing medical microdata, even though some mechanisms that
      implement k-anonymity have been demonstrated to be reversible [32].
   3. In the 2000s, research in theoretical computer science and cryptography developed
      the theory of differential privacy [53], which is based on a mathematical defnition
      of the privacy loss to an individual that results from queries on a database containing
      that individual’s personal information compared to the same queries on a database
      that does not.8 Differential privacy is termed a formal model for privacy protec-
      tion because its defnitions for privacy and privacy loss are based on mathematical
      proofs.9
       This does not mean that algorithms that implement differential privacy eliminate all
       privacy risk. Rather, it means that the amount of privacy risk that results from the
       use of these algorithms can be mathematically bounded. These mathematical limits
       on privacy risk have created considerable interest in differential privacy in academia,
       commerce, and business.
During the frst decade of the 21st century, there was a growing awareness within the U.S.
Government about the risks that could result from the improper handling and inadvertent
release of personal identifying and fnancial information. This realization, combined with

6 A summary of the history of statistical disclosure limitation can be found in Private Lives and Public Poli-

 cies: Confdentiality and Accessibility of Government Statistics [132].
7 k-anonymity is a refnement of the approach described by Dalenius in “Finding a needle in a haystack – or

 identifying anonymous census records” [36].
8 The term “privacy loss,” as defned in the literature of differential privacy, is neither a loss function nor a

  risk metric but an unbounded measure of information gain between two randomized queries – the ratio of
  two probabilities.
9 Other formal methods for privacy include cryptographic algorithms and techniques with provably secure

  properties, privacy-preserving data mining, Shamir’s secret sharing, and advanced database techniques. A
  summary of such techniques appears in [158].

                                                      10
