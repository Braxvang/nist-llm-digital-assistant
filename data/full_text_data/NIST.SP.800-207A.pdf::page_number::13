NIST SP 800-207A                                           ZTA Model for Access Control in Cloud-Native
September 2023                                               Applications in Multi-Location Environments


       The Enterprise Cloud-Native Platform and its Components
An enterprise cloud-native platform is increasingly made up of microservices that are
implemented as containers and hosted on a container orchestration platform. In addition, it has a
dedicated infrastructure layer called a service mesh, which provides a comprehensive set of
application services (e.g., network connectivity, network resilience, observability, and security).
These application services are enabled by the following:
   •   A built-in infrastructure for (a) providing service identities, (b) service discovery, and (c)
       external policy-based authorization engines in which policies incorporate contextual
       variables. Policies pertain to service-to-service and user-to-resource authentications and
       authorizations and ensure application integrity and confidentiality. These policies
       generally are expressed through a structure called access control. Some examples of
       access control models include: Next Generation Access Control (NGAC) model, and
       Attribute-based access control (ABAC) model.
   •   Code for performing network-related functions (e.g., traffic routing) and for ensuring
       network resiliency through functions such as retries, timeouts, blue-green deployments,
       and circuit breaking.
   •   More details on the container orchestration platform with an integrated service mesh can
       be found in [2], and an access control implementation in that platform is described
       extensively in [3].
In the modern enterprise, the platform described above is present in both on-premises data
centers and multiple cloud service locations. Assuming that a service mesh instance is deployed
for managing a single cluster that consists of the above platforms, there will be multiple clusters
spread over multiple on-premises sites and multiple availability zones in different clouds.
Consequently, there will be multiple service mesh instances.
Each service mesh instance has two main logical components: 1) a control plane that implements
the APIs needed to define various configurations and policies that govern access between various
microservices in that cluster and 2) a data plane that enforces those policies at runtime. However,
a uniform set of policies is also needed to govern access between any pair of microservices or
services in the enterprise irrespective of their location or the service mesh instance of which they
are a part. This requires a global control plane that can define a uniform set of policies applicable
to the entire set of services that operate in the enterprise and disseminate them to the control
planes of the individual service mesh instances.
It is technically possible to have a single service mesh control plane instance (i.e., single service
mesh instance) that manages multiple clusters spanning multiple environments (i.e., on-premises
and on clouds). However, this architecture may make the multiple clusters a single failure
domain and potentially defeat the very purpose of designing a multi-cluster configuration (i.e.,
availability). Thus, running a service mesh control plane instance for each cluster isolates the
failure domain and improves availability and scalability. Further, providing the required
underlying network connectivity to facilitate every workload (since each workload or application
instance has an associated sidecar proxy that forms the data plane) to communicate with a single



                                                  5
