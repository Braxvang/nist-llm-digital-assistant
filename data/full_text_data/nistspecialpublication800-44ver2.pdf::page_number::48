                                                                       GUIDELINES ON SECURING PUBLIC WEB SERVERS



as usernames and passwords or hidden server resources in URIs is not recommended. Security through
obscurity is not secure.

URIs are often included with public Web content. Although these URIs may not display as Web content
in a user’s Web browser, they can be easily discovered in the source code. Therefore, no publicly served
Web content should include sensitive URIs hidden in the source code. Many attackers and malicious bots
(see Section 5.2.4) search the source code for sensitive URI information, including—

 E-mail addresses

 Images on other servers

 Links to other servers

 Particular text expressions (e.g., userid, password, root, administrator)

 Hidden form values

 Hyperlinks.

A cookie is a small piece of information that may be written to the user’s hard drive when the user visits a
Web site. The intent of cookies is to allow servers to recognize a specific browser (user). In essence,
they add state to the stateless HTTP protocol. Because cookies are usually sent in the clear and stored in
the clear on the user’s host, they are vulnerable to compromise. There are known vulnerabilities in
certain versions of Internet Explorer, for example, that allow a malicious Web site to remotely collect all
of a visitor’s cookies without the visitor’s knowledge. Therefore, cookies should never contain data that
can be used directly by an attacker (e.g., username, password). OMB M-00-13 27 explicitly states that
Federal Web sites should not use cookies unless there is a compelling need to gather the data on the site,
and only with the appropriate approvals, notifications, and safeguards in place. For Web sites that need to
maintain session information, the session identifier can be passed as part of the URL to the Web site
rather than stored as a cookie. Regardless of whether cookies are being used or not, SSL/TLS should be
used to prevent attackers from retrieving information from the HTTP messages sent over the network and
using it to hijack a user’s session.

5.2.4    Controlling Impact of Web “Bots” on Web Servers

Web bots (also known as crawlers or spiders) are software applications used to collect, analyze, and index
Web content. Web bots are used by numerous organizations for many purposes. Some examples
include—

 MSNBot, Slurp, and Googlebot slowly and carefully analyze, index, and record Web sites for Web
  search engines such as Windows Live Search, Yahoo! and Google.

 Mediabot is used by Google to analyze content served by an AdSense page so that contextually
  relevant ads will be supplied.

 Hyperlink “validators” are used by Webmasters to automatically validate the hyperlinks on their Web
  site.


27
     OMB M-00-13, Office of Management and Budget Memorandum 2000-13, 2000, is available at
     http://www.whitehouse.gov/omb/memoranda/m00-13.html.


                                                        5-6
