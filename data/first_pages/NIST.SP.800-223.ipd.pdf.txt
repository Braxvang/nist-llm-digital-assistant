                             Withdrawn Draft

                                       Warning Notice

The attached draft document has been withdrawn and is provided solely for historical purposes.
It has been followed by the document identified below.

                            Withdrawal Date February 9, 2024

                        Original Release Date February 6, 2023




                        The attached draft document is followed by:

                  Status Final

         Series/Number NIST SP 800-223

                    Title High-Performance Computing Security: Architecture, Threat
                          Analysis, and Security Posture

       Publication Date February 2024

                    DOI https://doi.org/10.6028/NIST.SP.800-223

            CSRC URL https://csrc.nist.gov/pubs/sp/800/223/final

Additional Information
                     NIST Special Publication
                         NIST SP 800-223 ipd

 High-Performance Computing
              (HPC) Security:
Architecture, Threat Analysis, and Security Posture

                                             Initial Public Draft

                                                     Yang Guo
                                     Ramaswamy Chandramouli
                                               Lowell Wofford
                                                 Rickey Gregg
                                                     Gary Key
                                                 Antwan Clark
                                              Catherine Hinton
                                                 Andrew Prout
                                                Albert Reuther
                                                Ryan Adamson
                                                  Aron Warren
                                        Purushotham Bangalore
                                                 Erik Deumens
                                                  Csilla Farkas

                     This publication is available free of charge from:
                        https://doi.org/10.6028/NIST.SP.800-223.ipd
                                                     NIST Special Publication
                                                         NIST SP 800-223 ipd

            High-Performance Computing
                         (HPC) Security:
          Architecture, Threat Analysis, and Security Posture


                                                                                   Initial Public Draft

                 Yang Guo                         Antwan Clark                                Aron Warren
Ramaswamy Chandramouli           Laboratory for Physical Sciences               Sandia National Laboratories
 Computer Security Division
    Information Technology                   Catherine Hinton                        Purushotham Bangalore
                Laboratory       Los Alamos National Laboratory                         University of Alabama

           Lowell Wofford                          Andrew Prout                                 Erik Deumens
          Amazon.com, Inc.                        Albert Reuther                           University of Florida
                                           MIT Lincoln Laboratory
             Rickey Gregg                                                                        Csilla Farkas
                Gary Key                        Ryan Adamson                      University of South Carolina
                  HPCMP           Oak Ridge National Laboratory


                                                     This publication is available free of charge from:
                                                        https://doi.org/10.6028/NIST.SP.800-223.ipd

                                                                                                 February 2023




                                                                                      U.S. Department of Commerce
                                                                                       Gina M. Raimondo, Secretary

                                                                      National Institute of Standards and Technology
                   Laurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology
NIST SP 800-223 ipd (Initial Public Draft)                                   High-Performance Computing Security
February 2023

Certain commercial entities, equipment, or materials may be identified in this document in order to describe an
experimental procedure or concept adequately. Such identification is not intended to imply recommendation or
endorsement by the National Institute of Standards and Technology (NIST), nor is it intended to imply that the
entities, materials, or equipment are necessarily the best available for the purpose.

There may be references in this publication to other publications currently under development by NIST in
accordance with its assigned statutory responsibilities. The information in this publication, including concepts and
methodologies, may be used by federal agencies even before the completion of such companion publications. Thus,
until each publication is completed, current requirements, guidelines, and procedures, where they exist, remain
operative. For planning and transition purposes, federal agencies may wish to closely follow the development of
these new publications by NIST.

Organizations are encouraged to review all draft publications during public comment periods and provide feedback
to NIST. Many NIST cybersecurity publications, other than the ones noted above, are available at
https://csrc.nist.gov/publications.


Authority
This publication has been developed by NIST in accordance with its statutory responsibilities under the Federal
Information Security Modernization Act (FISMA) of 2014, 44 U.S.C. § 3551 et seq., Public Law (P.L.) 113-283.
NIST is responsible for developing information security standards and guidelines, including minimum requirements
for federal information systems, but such standards and guidelines shall not apply to national security systems
without the express approval of appropriate federal officials exercising policy authority over such systems. This
guideline is consistent with the requirements of the Office of Management and Budget (OMB) Circular A-130.

Nothing in this publication should be taken to contradict the standards and guidelines made mandatory and binding
on federal agencies by the Secretary of Commerce under statutory authority. Nor should these guidelines be
interpreted as altering or superseding the existing authorities of the Secretary of Commerce, Director of the OMB, or
any other federal official. This publication may be used by nongovernmental organizations on a voluntary basis and
is not subject to copyright in the United States. Attribution would, however, be appreciated by NIST.


NIST Technical Series Policies
Copyright, Use, and Licensing Statements
NIST Technical Series Publication Identifier Syntax


Publication History
Approved by the NIST Editorial Review Board on YYYY-MM-DD [will be add upon final publication]


How to Cite this NIST Technical Series Publication:
Guo Y, Chandramouli R, Wofford L, Gregg R, Key G, Clark A, Hinton C, Prout A, Reuther A, Adamson R, Warren
A, Bangalore P, Deumens E, Farkas C (2023) High-Performance Computing (HPC) Security: Architecture, Threat
Analysis, and Security Posture. (National Institute of Standards and Technology, Gaithersburg, MD), NIST Special
Publication (SP) NIST SP 800-223 ipd. https://doi.org/10.6028/NIST.SP.800-223.ipd


Author ORCID iDs
Yang Guo: 0000-0002-3245-3069
Ramaswamy Chandramouli: 0000-0002-7387-5858
Lowell Wofford: 0000-0003-1003-5090
Albert Reuther: 0000-0002-3168-3663
Aron Warren: 0000-0002-5090-2198
Purushotham Bangalore: 0000-0002-1098-9997
NIST SP 800-223 ipd (Initial Public Draft)                            High-Performance Computing Security
February 2023

Erik Deumens: 0000-0002-7398-3090


Public Comment Period
February 6, 2023 – April 7, 2023


Submit Comments
sp800-223-comments@list.nist.gov

National Institute of Standards and Technology
Attn: Computer Security Division, Information Technology Laboratory
100 Bureau Drive (Mail Stop 8930) Gaithersburg, MD 20899-8930


All comments are subject to release under the Freedom of Information Act (FOIA).
     NIST SP 800-223 ipd (Initial Public Draft)                       High-Performance Computing Security
     February 2023



 1   Abstract
 2   Security is an essential component of high-performance computing (HPC). HPC systems often
 3   differ based on the evolution of their system designs, the applications they run, and the missions
 4   they support. An HPC system may also have its own unique security requirements, follow
 5   different security guidance, and require tailored security solutions. Their complexity and
 6   uniqueness impede the sharing of security solutions and knowledge. This NIST Special
 7   Publication aims to standardize and facilitate the information and knowledge-sharing of HPC
 8   security using an HPC system reference model and key components as the basics of an HPC
 9   system lexicon. This publication also analyzes HPC threats, considers current HPC security
10   postures and challenges, and makes best-practice recommendations.

11   Keywords
12   high-performance computing (HPC); HPC security; HPC reference model; security guidance;
13   HPC threat analysis; HPC security posture.

14   Reports on Computer Systems Technology
15   The Information Technology Laboratory (ITL) at the National Institute of Standards and
16   Technology (NIST) promotes the U.S. economy and public welfare by providing technical
17   leadership for the Nation’s measurement and standards infrastructure. ITL develops tests, test
18   methods, reference data, proof of concept implementations, and technical analyses to advance
19   the development and productive use of information technology. ITL’s responsibilities include the
20   development of management, administrative, technical, and physical standards and guidelines for
21   the cost-effective security and privacy of other than national security-related information in
22   federal information systems. The Special Publication 800-series reports on ITL’s research,
23   guidelines, and outreach efforts in information system security, and its collaborative activities
24   with industry, government, and academic organizations.




                                                      i
     NIST SP 800-223 ipd (Initial Public Draft)                         High-Performance Computing Security
     February 2023

25   Call for Patent Claims
26   This public review includes a call for information on essential patent claims (claims whose use
27   would be required for compliance with the guidance or requirements in this Information
28   Technology Laboratory (ITL) draft publication). Such guidance and/or requirements may be
29   directly stated in this ITL Publication or by reference to another publication. This call also
30   includes disclosure, where known, of the existence of pending U.S. or foreign patent applications
31   relating to this ITL draft publication and of any relevant unexpired U.S. or foreign patents.
32   ITL may require from the patent holder, or a party authorized to make assurances on its behalf,
33   in written or electronic form, either:
34       a) assurance in the form of a general disclaimer to the effect that such party does not hold
35          and does not currently intend holding any essential patent claim(s); or
36       b) assurance that a license to such essential patent claim(s) will be made available to
37          applicants desiring to utilize the license for the purpose of complying with the guidance
38          or requirements in this ITL draft publication either:
39              i.    under reasonable terms and conditions that are demonstrably free of any unfair
40                    discrimination; or
41             ii.    without compensation and under reasonable terms and conditions that are
42                    demonstrably free of any unfair discrimination.
43   Such assurance shall indicate that the patent holder (or third party authorized to make assurances
44   on its behalf) will include in any documents transferring ownership of patents subject to the
45   assurance, provisions sufficient to ensure that the commitments in the assurance are binding on
46   the transferee, and that the transferee will similarly include appropriate provisions in the event of
47   future transfers with the goal of binding each successor-in-interest.
48   The assurance shall also indicate that it is intended to be binding on successors-in-interest
49   regardless of whether such provisions are included in the relevant transfer documents.
50   Such statements should be addressed to: sp800-223-comments@list.nist.gov




                                                       ii
     NIST SP 800-223 ipd (Initial Public Draft)                                                    High-Performance Computing Security
     February 2023

51   Table of Contents
52          Introduction ......................................................................................................................... 1
53          HPC System Reference Model and Main Components.................................................... 2
54                 Main Components ......................................................................................................... 3
55      2.1.1.       Components of the High-Performance Computing Zone ............................................ 3
56      2.1.2.       Components of the Data Storage Zone ...................................................................... 4
57      2.1.3.       Components of the Access Zone ................................................................................ 5
58      2.1.4.       Components of the Management Zone ...................................................................... 6
59      2.1.5.       HPC Software ............................................................................................................. 7
60      2.1.6.       Container Usage in HPC ............................................................................................. 8
61                 HPC Architecture Variants ............................................................................................ 9
62      2.2.1.       Diskless Booting HPC ................................................................................................. 9
63      2.2.2.       Virtual and Hybrid HPC Environments ........................................................................ 9
64          HPC Threat Analysis ......................................................................................................... 11
65                 Key HPC Security Characteristics and Use Requirements ......................................... 11
66                 Threats to HPC Function Zones .................................................................................. 11
67      3.2.1.       Access Zone Threats ................................................................................................ 11
68      3.2.2.       Management Zone Threats ....................................................................................... 12
69      3.2.3.       High-Performance Computing Zone Threats ............................................................ 12
70      3.2.4.       Data Storage Zone Threats ...................................................................................... 13
71                 Other Threats .............................................................................................................. 13
72          HPC Security Posture, Challenges, and Recommendations ........................................ 15
73                 HPC Access Control via Multiple Physical Networks .................................................. 15
74                 Compute Node Sanitization......................................................................................... 16
75                 Data Integrity Protection .............................................................................................. 16
76                 Securing Containers .................................................................................................... 17
77                 Achieving Security While Maintaining HPC Performance ........................................... 17
78                 Challenges to HPC Security Tools .............................................................................. 17
79          Conclusions ....................................................................................................................... 19
80   References ................................................................................................................................. 20


81   List of Figures
82   Fig. 1. HPC System Reference Model .......................................................................................... 3
83




                                                                           iii
      NIST SP 800-223 ipd (Initial Public Draft)                      High-Performance Computing Security
      February 2023



 84       Introduction
 85   In 2015, Executive Order 13702 established the National Strategic Computing Initiative (NSCI)
 86   to maximize the benefits of high-performance computing (HPC) for economic competitiveness
 87   and scientific discovery. The ability to process large volumes of data and perform complex
 88   calculations at high speeds is a key part of the Nation’s vision for maintaining its global
 89   competitive edge.
 90   Security is an essential to achieving the anticipated benefits of HPC. HPC systems bear some
 91   resemblance to ordinary IT computing, which allows for the effective application of traditional
 92   IT security solutions. However, they also have significant differences. An HPC is designed to
 93   maximize performance so its architecture, hardware components, software stacks, and working
 94   environment are very different from ordinary IT. As such, security solutions must be tailored to
 95   the HPC system’s requirements. Furthermore, HPC systems are often different from one another
 96   due to the evolution of their system designs, the applications they run, and the missions they
 97   support. An HPC system frequently has its own unique security requirements and follows
 98   different security guidance, which can impede the sharing of security solutions and knowledge.
 99   This NIST Special Publication aims to standardize and facilitate the sharing of HPC security
100   information and knowledge through the development of an HPC system reference model and key
101   components, which are introduced as the basics of the HPC system lexicon. The reference model
102   divides an HPC system into four function zones. A zone based on the HPC reference model
103   captures the most common features across the majority of HPC systems and segues into HPC
104   system threat analysis. Key HPC security characteristics and use requirements are laid out
105   alongside the major threats faced by the system and individual function zones. HPC security
106   postures, challenges, and recommendations are also included.




                                                     1
      NIST SP 800-223 ipd (Initial Public Draft)                      High-Performance Computing Security
      February 2023

107       HPC System Reference Model and Main Components
108   The HPC system is complex and evolving so a common lexicon can help describe and identify
109   an HPC system’s architecture, critical elements, security threats, and potential risks. An HPC
110   system is divided into four function zones:
111       1. The high-performance computing zone consists of a pool of compute nodes connected by
112          one or more high-speed networks. The high-performance computing zone provides key
113          services specifically designed to run parallel jobs at scale.
114       2. The data storage zone comprises one or multiple high-speed parallel file systems that
115          provide data storage service for user data. The high-speed parallel file systems are
116          designed to store very large data sets and provide fast access to data for reading and
117          writing.
118       3. The access zone has one or more nodes that are connected to external networks, such as
119          the broader organizational network or the internet. This zone provides the means for
120          authenticating and authorizing the access and connections of users and administrators.
121          The access zone provides various services, including interactive shells, web-based
122          portals, data transfer, data visualization, and others.
123       4. The management zone comprises multiple management nodes and/or cloud service
124          clusters through which HPC management services are provided. The management zone
125          allows HPC system administrators to configure and manage the HPC system, including
126          the configuration of compute nodes, storage, and networks, provisioning, identity
127          management, auditing, system monitoring, and vulnerability assessment. It also offers,
128          through a portal in the access zone, users an interface to acquire high-performance
129          computing services and to configure access to data storage services. Various management
130          software modules – from job scheduler, workflow management, and the Domain Name
131          System (DNS) – run in the management zone.
132   The HPC system reference model is depicted in Figure 1.
133




                                                      2
      NIST SP 800-223 ipd (Initial Public Draft)                          High-Performance Computing Security
      February 2023




134
135                                       Fig. 1. HPC System Reference Model


136           Main Components

137   2.1.1. Components of the High-Performance Computing Zone
138   An HPC cluster consists of a collection of independent computing systems, called compute
139   nodes, which are interconnected via high-speed networks. Compute nodes have the same
140   components as a laptop or desktop, including central processing unit (CPU) cores, memory, disk
141   space, and networking interface cards. However, they are equipped with a much larger number
142   of CPU cores and more memory than a laptop or desktop. In some HPC architectures, a compute
143   node may not have local disks and uses the data storage services of remote storage servers
144   instead. In addition, there may be different types of nodes for different types of tasks, and some
145   compute nodes are equipped with hardware accelerators to speed up specific applications. For
146   instance, compute nodes often utilize graphics processing units (GPUs) [1] to accelerate
147   modeling and simulation or AI and machine learning (ML) model training.
148   An HPC compute node installs its own software stack (e.g., operating system [OS], compilers,
149   software libraries, etc.) to support applications. The installation and configuration of the software
150   stacks are cluster-wide, centrally managed, and controlled by the management zone. The number
151   of compute nodes in an HPC ranges from a few nodes to hundreds and even thousands of nodes.
152   A critical requirement of HPC networking that interconnects computer nodes is to have massive
153   amounts of scalable bandwidth (throughput) while keeping the latency ultra-low so that the
154   compute nodes and parallel file system (PFS) in the data storage zone can work as one
155   supercomputer. HPC networking often employs specifically designed protocols, networking
156   cards, processor nodes, and switches to optimize network performance. The popular HPC
157   interconnect networking includes InfiniBand [2], Omni-Path [3], Slingshot [4], and others.
158   A high-performance computing zone typically utilizes non-high-performance communication
159   networks, like ethernet, as cluster internal networks that connect the high-performance


                                                          3
      NIST SP 800-223 ipd (Initial Public Draft)                         High-Performance Computing Security
      February 2023

160   computing zone with the management zone and access zone for traffic associated with
161   maintenance activities as opposed to HPC traffic.

162   2.1.2. Components of the Data Storage Zone
163   Several different classes of storage systems may be present inside of the data storage zone. In
164   general, storage systems within this zone cannot be effectively separated from the HPC resources
165   that they support from an administrative privilege perspective. Typical classes of storage found
166   within this zone include parallel file systems (PFS), node-local storage for low-latency
167   workloads, and archival file systems that support campaign storage and protect against data loss.
168   HPC systems may have other file systems that store non-user data. For instance, the management
169   zone often has its own file system that stores the OS images and configuration files. In that case,
170   the file system is included in the corresponding function zone.
171   HPC applications’ initial data, intermediate results, and final results are stored in the data storage
172   zone and can be accessed during the application runtime and after the application’s completion.
173   External HPC users can also access user data through the login nodes and/or data transfer nodes
174   in the access zone.
175   The storage capacity of these file systems is often measured in petabytes and can reach up to
176   exabytes. File systems within the data storage zone will generally use a transport mechanism
177   appropriate to the tier. For example, high-bandwidth file systems may be attached to the HPC
178   resource’s high-performance network, while lower bandwidth file systems may use 10 Gbps or
179   100 Gbps ethernet. Access control for most HPC file systems is enforced by the operating system
180   software of nodes on which these file systems are mounted. As such, file systems should not be
181   mounted outside of their security boundaries. A rogue system that can mount a file system will
182   have complete control of all file system data, can spoof packets on the high-speed network, and
183   can possibly gain privileges elsewhere within other zones of the HPC security enclave.

184   2.1.2.1.         Parallel File System
185   Since HPC workloads can vary significantly, a parallel file system (PFS) is often required to
186   support read-intensive and write-intensive applications with sequential and random-access
187   patterns at speeds of up to terabytes per second. Commonly seen file systems include Lustre [5],
188   GPFS [6], and IBM Spectrum Scale [7]. During procurement, a PFS will typically be designed to
189   hit a particular aggregate bandwidth target rather than a capacity requirement. These PFS will
190   typically consist of a cluster of systems to maintain metadata about files and locations as well as
191   servers that act as storage targets. Clients that mount the file system will typically load the file
192   system client software via a kernel module. Storage target servers will have backing storage
193   arrays configured with dozens of disks in a redundant array of inexpensive disk (RAID) strategy.
194   Both GPFS and Lustre-based PFS are prone to performance degradation when a certain capacity
195   threshold is reached. These file systems may be regularly pruned of unwanted files with a
196   strategy decided by the file system administrators. Some deployments will sweep files older than
197   a certain age, which forces HPC users to transfer job output to a longer-term file system, such as
198   campaign storage. PFS tends to be somewhat unreliable depending on the types of activities
199   being performed by running jobs and users. Because these are distributed file systems, file-
200   system software must solve distributed locking of files to ensure deterministic file updates when


                                                        4
      NIST SP 800-223 ipd (Initial Public Draft)                        High-Performance Computing Security
      February 2023

201   multiple clients are writing to the same file at once. Additionally, PFS are susceptible to denial-
202   of-service conditions even during legitimate user operations, such as listing a directory with
203   millions of files or applications that perform poor file locking semantics.

204   2.1.2.2.         Archival and Campaign Storage
205   Archival and campaign storage systems represent a class of storage that is more resilient to
206   failure conditions than PFS and is often less expensive per GigaByte. These advantages come at
207   the cost of bandwidth and an increased latency of data transfer. While PFS acts as a temporary
208   short-term scratch file system, campaign storage supports the longer-term storage needs of a
209   project over its life cycle. Finished data products that support scientific publications or other
210   high-value datasets may also be stored in an archival file system. The retention time for data on a
211   campaign storage file system is measured in years, while the retention of data within an archival
212   storage system is measured in decades. Both campaign and archival storage systems might
213   employ low-latency disks – such as solid state drives (SSD) or non-volatile memory express
214   (NVMe) drives within a small tier of storage that acts as a cache – and are backed with cheaper,
215   higher capacity media, such as spinning disk and/or tape media.

216   2.1.2.3.         Burst Buffer
217   For applications that require extremely low latency or high-bandwidth memory-to-disk data
218   transfer during runtime, intermediate storage layers that contain “burst buffers” have been
219   incorporated as brokers to primarily mitigate the effects of input/output (I/O) contention and the
220   bandwidth burden on parallel file system (PFS). These burst buffers can pre-fetch data from the
221   parallel file system (PFS) before a computing job begins and stage data out to a parallel file
222   system after a computing job has completed. This saves job runtime that would normally be
223   spent performing bulk I/O to the PFS and allows it to be spent on computation instead. Typical
224   HPC infrastructures contain the following intermediate storage architectures:
225       •   Node-local burst buffer architectures: Each burst buffer is collocated with a
226           corresponding HPC compute node [8]. This is advantageous for its scalability and also
227           improves the checkpoint bandwidth because the aggregate bandwidth increases in
228           proportion with the number of compute nodes.
229       •   Remote-shared burst buffer architectures: Burst buffers are shared between multiple
230           HPC compute nodes that are hosted on an I/O node [8]. This is advantageous for
231           facilitating the development, deployment, and maintenance of these architectures.
232   There are also HPCs that can contain mixed burst buffer intermediate storage architectures,
233   which are a mixture of the strengths of node-local and remote-shared burst buffer architectures.

234   2.1.3. Components of the Access Zone
235   A typical HPC system provides one or more nodes through which users and administrators
236   access the system. At least one of these nodes is a login node where users have access to shells to
237   launch interactive or batch jobs. Some of these login nodes may also have specialized
238   visualization hardware and software with which users can conduct interactive and/or post-
239   execution visualization of their datasets. There may also be one or more data transfer nodes that


                                                       5
      NIST SP 800-223 ipd (Initial Public Draft)                      High-Performance Computing Security
      February 2023

240   provide services to transfer data into and out of the HPC system and may even provide storage-
241   mounting services like Network File System (NFS) [9], Common Internet File System (CIFS)
242   [10], Server Message Block (SMB) [11], and Filesystem in Userspace (FUSE) [12] based SSH
243   Filesystem (SSHFS) [13]. Many HPC systems now provide web portals via web portal nodes
244   that enable a variety of web interfaces to HPC system services.

245   2.1.4. Components of the Management Zone
246   The complexity of HPC systems requires a significant infrastructure to operate and manage it,
247   which is collectively referred to as the management zone. The management zone may consist of
248   servers and network switches that enable various functions for operating the system with
249   efficiency, effectiveness, and stability.

250   2.1.4.1.         General Architecture and Characteristics
251   One important characteristic of the management zone is that it has a separate security posture
252   because non-privileged users do not need to access the management servers or services in a
253   direct way. Privileged users responsible for configuring, maintaining, and operating the HPC
254   system access the management zone servers and switches through extra security controls. For
255   example, from the public-facing login nodes, they may go through a bastion host that is typically
256   located in the management zone, or they may establish a private virtual network (VPN) with
257   separate authentication and authorization or other appropriate security controls to reach the
258   management zone. All systems are configured on networks that are not routed beyond the
259   perimeter of the HPC system so that only nodes like compute nodes and storage nodes can access
260   the services.
261   The services provided by the management zone have clearly defined protocols and can be
262   implemented as running on assigned hardware platforms or run as virtual machines on a
263   dedicated set of hardware resources. The fact that the management zone has a clear and separate
264   security posture helps with risk assessment and the selection of controls to secure the
265   management zone and manage the risk.

266   2.1.4.2.         Basic Services
267   The HPC resources inside of the computing and data storage zones need various services to
268   operate. Examples include Domain Name Services (DNS) [14]; the Dynamic Host Configuration
269   Protocol (DHCP) [15]; configuration definitions, authentication, and authorization services, such
270   as those provided by an LDAP server [16]; and the Network Time Protocol (NTP) [17] for
271   synchronization, log management, version-controlled repositories.
272   The management zone includes storage systems to store configuration data and node images,
273   current versions, development and test versions, and historical versions. Storing logs from the
274   entire HPC system is also part of the management zone as well as the servers to process the logs
275   and alert administrators of events, problems, and incidents. Many of these services will be
276   implemented with high availability and failover capabilities to avoid failure of the HPC
277   resources. The network switches for the management network (ethernet) and the fast




                                                      6
      NIST SP 800-223 ipd (Initial Public Draft)                      High-Performance Computing Security
      February 2023

278   interconnects (e.g., InfiniBand, Omni-Path, Slingshot) are often managed as part of the
279   management zone because non-privileged users do not need direct access to these resources.

280   2.1.4.3.         Configuration Management
281   Automated configuration management is crucial to ensure the stable operation of complex
282   systems like HPC. The systems that hold the configuration database and run the server to place
283   configurations on compute nodes, storage servers, and network switches are part of the
284   management zone. The nodes are subject to a regularly scheduled process to verify configuration
285   and enforce consistency with what the configuration management nodes and databases specify.
286   Often, the configuration management systems in the management zone have an even more
287   restricted security posture than the management zone as a whole, with a smaller number of
288   privileged users having access.

289   2.1.4.4.         HPC Scheduler and Workflow Management
290   Because of the distributed nature of HPC systems, requesting resources for given workloads is
291   coordinated by a scheduler or workload manager, such as Slurm [18] and Kubernetes [19]. These
292   services are run on servers in the management zone alongside the configurations and job logs.
293   Non-privileged users access the scheduler through specific commands or an application
294   programming interface (API). Access to the service is restricted to nodes within the HPC system
295   perimeter. There may also be a web interface that provides a separate authenticated and
296   authorized path for scheduling workloads, often within the strict constraints of certain
297   application domains.

298   2.1.5. HPC Software
299   In addition to the management software that installs, boots, configures, and manages HPC-
300   related systems, HPC application codes rely on several layers of scientific and performance-
301   enhancing libraries. The layers of software that are available to users is referred to as the
302   software stack. The lower layers of this stack are typically focused on performance and include
303   compilers, communication libraries, and user-space interfaces to HPC hardware components.
304   The middle layer includes performance tools, math libraries, and data or computation abstraction
305   layers. The top of the stack consists of end-user science or production applications. Each
306   software product within this stack may require certain versions or variants of other products and
307   have many dependencies. For example, Hierarchical Data Format version 5 (HDF5) [20] is a
308   scientific data formatting library with only seven dependencies, while Data Mining Classification
309   and Regression Methods (rminer) [21] – an R-based data mining application – has 150 software
310   dependencies. The full software stack can be split into three general categories that differ based
311   on the maintainer: user software, facility software, and vendor software.

312   2.1.5.1.         User Software
313   Often, the end users themselves best understand how to tune their software to the bespoke
314   hardware of an HPC system to ensure sufficient performance for their workload. Users regularly
315   modify and recompile their software to enhance performance, fix bugs, and adapt to changes in


                                                      7
      NIST SP 800-223 ipd (Initial Public Draft)                       High-Performance Computing Security
      February 2023

316   the underlying dependencies or kernel interfaces over time. The sharing of user-built software
317   between teams may be common. User software that is widely used is often open source and,
318   therefore, subject to open-source software supply chain concerns.
319   Continuous integration (CI) pipelines [22] and tests of scientific code on HPC platforms have
320   recently become commonplace. Industry-standard identification of software weaknesses and the
321   publication of Common Vulnerabilities and Exposures (CVEs) [23] is not routine, but the
322   identification and remediation of performance regressions is generally a higher priority within
323   the user community. There is a value-per-cycle trade-off for CI tests since cheaper cycles on
324   commodity hardware may not expose bugs on much more expensive HPC resources. Complex
325   test suites will eat into user allocations, and users and staff prefer that only a cardinal set of
326   smoke tests run within user-developed testing pipelines on HPC systems.

327   2.1.5.2.         Site-Provided Software and Vendor Software
328   Site staff and administrators generally build applications and libraries that are most likely to be
329   used. Tools such as Conda [24], EasyBuild [25], and Spack [26] are often used to manage the
330   complexity of software dependency resolution. Staff may also wrap compiler and job submission
331   utilities with custom scripts to collect usage information about software libraries, I/O read and
332   write patterns, or other system telemetry that is useful for decision making.
333   Vendor software includes low-level system tools to facilitate the running of other software. For
334   instance, remote direct memory access, inter-node memory sharing, performance counters,
335   temperature and power telemetry, and debugging are all vendor-provided software.
336   Users can choose specific versions of installed vendor and site-provided software libraries by
337   manipulating environment variables. Tools such as wrapper scripts or module files are usually
338   provided to help users find and choose which versions of installed software to use.

339   2.1.6. Container Usage in HPC
340   A container is a software package that contains an application’s entire runtime environment. It
341   consists of an application program and all of the dependencies, libraries, other binaries, and
342   configuration files needed to run it. Containers provide self-contained, portable, and reproducible
343   environments that abstract away the differences in OS distributions and underlying
344   infrastructure. Containers make applications more portable and the deployment easier. For
345   instance, containers allow users to use a package manager (e.g., apt [27] or yum [28]) to install
346   software without changing anything on the host system or to run the latest software built for
347   newer Linux OS versions.
348   Security is a major concern in deploying containers in HPC environments. Containers possess
349   large attack surfaces due to the different underlying images, each of which can have
350   vulnerabilities. In addition, securing the host is not enough to ensure protection. Container
351   permissions and proper isolation are also necessary. Finally, monitoring containers can be
352   difficult due to its dynamic nature.




                                                       8
      NIST SP 800-223 ipd (Initial Public Draft)                        High-Performance Computing Security
      February 2023

353           HPC Architecture Variants

354   2.2.1. Diskless Booting HPC
355   A variant of HPC cluster design is diskless booting clusters in which the OS is not loaded from
356   storage located on the node itself. Diskless nodes typically boot up, obtain an IP address via
357   DHCP, and receive a kernel and partial OS over the network through a protocol, such as the
358   Preboot eXecution Environment (PXE) [29] . To finish the boot process, the remaining OS is
359   typically loaded over NFS or Internet Small Computer System Interface (ISCSI) [30] .
360   For scalability, some HPC clusters may implement a hierarchy design in which intermediate
361   servers boot diskless but have local storage that is used to serve the OS to leaf nodes.

362   2.2.2. Virtual and Hybrid HPC Environments
363   Virtualization technologies can be used to recreate and maintain traditional HPC architectures
364   that allow them to leverage the virtual infrastructure effectively while also providing additional
365   scalability. For example, virtual machines (VMs) can be used to redistribute scientific
366   applications across heterogeneous supercomputing architectures that can be bundled more easily
367   without the need for certain special privileges. Virtual HPC (vHPC) architectures typically
368   contain the following main components [31]:
369       •   Hypervisors: These elements create and run VMs in vHPC architectures, which allows
370           them to use multiple resources, such as memory and processing. The benefits to these
371           elements include their speed, efficiency, flexibility, convenience, and portability.
372       •   vCenter Server Appliance (VCSA): This element provides centralized management of
373           all virtualized infrastructure with a single management interface that interacts, manages,
374           and monitors the virtualization configuration, settings, and services.
375       •   Network virtualization and security platform: This platform provides software-
376           defined networking (SDN) management and protection capabilities to the vHPC
377           infrastructure.
378   Some optional technologies include Kubernetes-based platforms and technologies that accelerate
379   the deployment and management of applications and services, optimize systems and application
380   environments, and securely connect and operate remotely. vHPC architectures also contain
381   access, management, compute, and data storage zones, as highlighted in Figure 1, in which the
382   management zones contain VMs that manage the vHPC environment. The compute zones
383   contain virtual compute clusters that are responsible for HPC workloads, while the storage zones
384   consist of virtualized PFS capabilities that store user data. Virtual compute clusters can contain
385   low-latency burst buffer storage that is local to each node or shared across nodes.
386   Cloud technologies can also be employed to enhance traditional HPC infrastructures, which
387   result in the creation of hybrid HPC environments that contain a combination of traditional and
388   cloud HPC architectures [32]. These types of schemas typically use private cloud capabilities
389   because they are more flexible, customizable, and manageable than using public cloud
390   technologies. Recently, high-performance computing as a service (HPCaaS) has been adopted as
391   a flexible solution that offers similar supercomputing capabilities on the cloud. Organizations



                                                       9
      NIST SP 800-223 ipd (Initial Public Draft)                      High-Performance Computing Security
      February 2023

392   can work with vendors or service providers to customize these architectures to their specific
393   needs and maintain control by changing their capacities and architectures. HPCaaS is also cost-
394   effective because these solutions can typically be purchased via pay-as-you-go subscriptions that
395   are managed by their service providers.




                                                     10
      NIST SP 800-223 ipd (Initial Public Draft)                       High-Performance Computing Security
      February 2023

396       HPC Threat Analysis
397   HPC poses unique security and privacy challenges, and collaboration and resource-sharing are
398   integral. HPC workloads are often different from their traditional counterparts. For instance,
399   scientific experiments frequently employ unique hardware, software, and configurations that may
400   not be maintained or well-vetted or that present entirely new classes of vulnerabilities absent in
401   more traditional environments. HPC can store large amounts of sensitive research data,
402   personally identifiable information (PII), and intellectual property (IP) that need to be
403   safeguarded. Finally, HPC data and computation are encumbered with a variety of different
404   security and policy constraints. The solutions to protecting data, computation, and workflows
405   must balance these trade-offs.

406           Key HPC Security Characteristics and Use Requirements
407   HPC systems possess some unique security characteristics and distinctive use requirements that
408   differentiate themselves from average IT system:
409
410       •   Tussles between performance and security: HPC users may consider security valuable
411           only to the extent that it does not significantly slow down the HPC system and impede
412           research. Ensuring the usability of security mechanisms with a tolerable performance
413           penalty is therefore critical to adoption by the scientific HPC community.
414       •   Varying security requirements for different HPC applications: Individual platforms,
415           projects, and data may have significantly different security sensitivities and need to
416           follow different security policies. An HPC may need to enforce multiple security policies
417           simultaneously.
418       •   Limited resources for security tools: Most HPC systems are designed to devote their
419           resources to maximizing performance rather than acquiring and operating security tools.
420       •   Open-source software and self-developed research software: Open-source software
421           and self-developed research software are widely used in HPC. Open-source software is
422           vulnerable to open-source software supply chain threats, while HPC software input data
423           may also be vulnerable to data supply chain threats. Self-developed software is
424           susceptible to low software quality.
425       •   Granular access control on databases: Since different research groups may have a need
426           to know for different portions of data, granular access control capabilities are necessary.
427           Access control requirements may need to be dynamically adjusted as some scientific
428           experiments may increase data needs based on the outcome of experiments.

429           Threats to HPC Function Zones

430   3.2.1. Access Zone Threats
431   The access zone provides an interface for external users to access the HPC system and oversees
432   the authentication and authorization of users. Among the four function zones, the access zone is
433   the only one that is directly connected to the external networks. Hence, the nodes and their


                                                      11
      NIST SP 800-223 ipd (Initial Public Draft)                        High-Performance Computing Security
      February 2023

434   software stacks in this zone are susceptible to external attacks, such as denial of service (DoS)
435   attacks, perimeter network scanning and sniffing, authentication attacks (e.g., brute force login
436   attempts and password guessing), user session hijacking, and machine-in-the-middle attacks. In
437   addition, some nodes are subject to extra attacks due to their specific software stacks. For
438   instance, a web server may be subject to website defacement, phishing, misconfiguration, and
439   code injection attacks. The access zone also provides access to the file systems hosted in the data
440   storage zone. It is important that permissions to directories and files are only given to authorized
441   accesses.
442   Authenticated users sometimes use external networks to download data or code for use inside of
443   the HPC system, which introduces the risk of unintentionally downloading malicious content.
444   The nodes in the access zone are usually configured to support limited computation (e.g., modest
445   debugging). Access zone nodes are susceptible to computational resource abuse.
446   The access zone is also shared by multiple users. One user’s activities, such as commands issued
447   and jobs submitted, can be viewed by other users. A port opened by one user can potentially be
448   used by others. Fortunately, the nodes in the access zone work similarly to enterprise servers, and
449   general IT security tools and measures are available to harden the zone.

450   3.2.2. Management Zone Threats
451   The management zone is responsible for managing the entire HPC system. It is connected to
452   clustered internal networks through which other zones can be reached. It runs a plethora of
453   system management, out-of-band hardware management, job scheduling, and workflow
454   management software, all of which are susceptible to unique threats.
455   Processes running in the management zone, such as schedulers and data tiering/orchestration
456   processes, act on behalf of users. These are privileged processes, and if they are spoofed, it can
457   lead to privilege escalation, which is a distributed system-to-system trust problem. Due to the
458   implied delegation of authority within distributed HPC and file systems, ‘root’ on a compute
459   node may be, depending on configuration, equivalent to ‘root’ on all systems within the HPC
460   zones. Only administrators with privileged access authorization are allowed to log into the
461   management zone, where a privileged administrator logs into the access zone first and then logs
462   into the management zone. A malicious user may attempt to log into the management zone.
463   The management zone may also be implemented as a service running on a cloud via
464   virtualization technologies. In such cases, the risks associated with the cloud also apply to the
465   management zone.

466   3.2.3. High-Performance Computing Zone Threats
467   The high-performance computing zone offers core computational functions in an HPC system.
468   The computer nodes are shared by multiple users or tenants. The exploitation of multi-tenancy
469   environments is a major threat (e.g., side-channel attacks, user data/program leakage, etc.). Other
470   threat sources that often cause extreme resource consumption, performance degradation, or the
471   outage of the HPC system entirely include accidental misconfiguration, software bugs introduced
472   by user-developed software, and system abuse by running applications that are not aligned with
473   the HPC mission. Container escape, side-channel attacks, and DoS can also be threats if



                                                       12
      NIST SP 800-223 ipd (Initial Public Draft)                         High-Performance Computing Security
      February 2023

474   virtualization technologies – such as containers – are used in the high-performance computing
475   zone.
476   As a security mitigating technology, the applications in HPC are mostly run in the user space,
477   except for system calls that must run in the kernel with elevated privilege. Accelerators, high-
478   performance interconnects, special protocols, and direct memory access between nodes are
479   commonly used in the high-performance computing zone. Some of these technologies may not
480   be thoroughly tested, and their speed, novelty, and complexity can make monitoring and
481   detecting suspicious activity difficult. Direct memory access and communication between nodes
482   may bypass the kernel and the protections provided by the kernel (e.g., Security-Enhanced Linux
483   [SELinux] [33]) is lost.

484   3.2.4. Data Storage Zone Threats
485   Protecting the confidentiality and integrity of user data is essential for the data storage zone. Data
486   integrity can be compromised by malicious data deletion, corruption, pollution, or false data
487   injection so gaining unauthorized privileged access is a major threat. Legitimate users may also
488   mishandle sensitive data, leading to confidentiality breakdown. File metadata (e.g., file name,
489   author, size, creation date) can also leak sensitive information about the files.
490   HPC file systems in the data storage zone provide superior data access speed and much larger
491   storage size compared to average enterprise file systems. Hard disk failure is a threat due to the
492   large number of disks deployed in the data storage zone. Incident response and contingency
493   planning controls may not be easy to implement, and file backup, recovery, and forensic imaging
494   may become infeasible. The security measures that can be implemented on the enterprise file
495   systems may take an unacceptably long time and degrade HPC file system performance in an
496   unacceptable way.
497   Providing data backup services is another challenge in HPC due to its large volume. By default,
498   user data is often not backed up, and users are responsible for maintaining their own data copies.
499   Inadvertent operations (e.g., accidently deleting a file subdirectory) can cause the permanent loss
500   of data, though making data READ ONLY is one way to combat such a risk. Some organizations
501   offer backup services using their own HPC systems, but these systems may be in the same
502   geographic locations and subject to the same environmental threats.

503           Other Threats
504   In addition to the threats unique to individual function zones, the general HPC systems face the
505   following threats:
506       • Environmental and physical threats: Physical or cyber attacks against facilities (e.g.,
507           power, cooling, water), unauthorized physical access, and natural disaster (e.g., fire,
508           flood, earthquake, hurricane, etc.) are all potential threats to an HPC system.
509       •   Vulnerabilities introduced by prioritizing performance in HPC design and
510           operation: HPC is designed to process large volumes of data and perform complex
511           computations at very high speeds. Achieving the highest performance possible is a
512           priority in HPC design and operation. Such prioritization, however, has its security
513           implications. For instance, designers often make conscious decisions to build a less



                                                       13
      NIST SP 800-223 ipd (Initial Public Draft)                       High-Performance Computing Security
      February 2023

514           redundant system to achieve high performance, making the system less robust and
515           potentially more vulnerable to attacks, such as DoS attacks. As another example, using a
516           backup system to improve system robustness and high availability is a proven
517           technology. Building a backup HPC system, however, is often infeasible since it is too
518           costly. HPC often lacks storage backup due to the vast size of the data stored. Similarly,
519           most HPC missions do not have a service-level agreement to justify the need for a full
520           backup system at a backup location. All resources are poured into building the single best
521           HPC system possible.
522       •   Supply chain threats: The HPC supply chain faces a variety of threats, from the theft of
523           proprietary information to attacks on critical hardware components and software
524           manipulation to gain unauthorized access. Some HPC software (e.g., OS, BIOS,
525           applications), firmware, and hardware components have limited manufacturers, suppliers,
526           and integrators, which make diversification difficult. Limited suppliers also lead to
527           shortages in the qualified workforce who can perform required technical support.
528       •   Insider threats: Insider threats come from people within the organization who have
529           internal information and may have the privileges needed to access the HPC system.
530           Insider threats can be classified into accidental/unintentional threats and
531           malicious/intentional threats. Unintentional threats come from the unintended side effects
532           of normal actions and activity. In contrast, a malicious insider may intentionally upload
533           malicious code into the HPC system.




                                                      14
      NIST SP 800-223 ipd (Initial Public Draft)                       High-Performance Computing Security
      February 2023

534       HPC Security Posture, Challenges, and Recommendations

535           HPC Access Control via Multiple Physical Networks
536   Access control is a security technique that regulates who can access and/or use resources in a
537   computing environment. In HPC, multiple physical networks are constructed as an effective
538   means of access control:
539       •   Management network: The management network is a dedicated network that allows
540           system administrators to remotely control, monitor, and configure computer nodes in an
541           HPC system. Modern computers are often equipped with the Intelligent Platform
542           Management Interface (IPMI) [34], which provides management and monitoring
543           capabilities that are independent of the host system’s CPU, firmware (e.g., BIOS [35]or
544           Unified Extensible Firmware Interface [UEFI] [36]), and operating system. For example,
545           IPMI allows system administrators to remotely power on/off unresponsive machines and
546           install custom operating systems. IPMIs are connected to the management network,
547           which can only be accessed by authorized system administrators.
548       •   High-performance networks: High-performance networks offer high bandwidth and
549           low latency to connect computer nodes inside of the high-performance zone and data
550           storage zone. They also support features that are unique to HPC, such as remote memory
551           access over the network and the message passing interface (MPI) [37]. High-performance
552           networks often use special communications standards and architectures to achieve the
553           high performance (e.g., InfiniBand, Slingshot, Omni-Path, etc.).
554       •   Auxiliary networks: Additional auxiliary networks can be added to support usability and
555           system manageability. For instance, a user network is constructed to allow users to
556           manipulate or share data or remotely log into and access the compute nodes. Depending
557           on the purpose of the networks, a subset of nodes from different zones are selected to be
558           party to the networks. As an example, a user network contains the nodes in the access
559           zone and the computer nodes in the high-performance computing zone.
560   There are many benefits to having multiple networks in an HPC system. First, all of these
561   networks are private and use different IP address ranges. Network traffic will remain on one
562   network, which facilitates monitoring and measurement. Second, individual networks often serve
563   specific purposes so only the relevant nodes are connected to the network. The networks
564   effectively segment the HPC system into smaller segments, which improves security. Finally,
565   multiple physical networks also provide a degree of fault tolerance. When one network goes
566   down, the system administrator can use the other network to diagnose and troubleshoot.
567   The compute nodes in the access zone are connected to the external network and assigned public
568   IP addresses, which allow users to remotely access the HPC systems. The user data can be shared
569   through the login nodes or the data transfer nodes. Some systems allow storage to be exported
570   using CIFS [10] or SMB [11] (e.g. via a SAMBA [38] server). If necessary, a network address
571   translation (NAT) [39] or a Squid proxy [40] can be installed to allow users on a private network
572   to access the internet and download new versions of software or share software data. However, a
573   NAT and Squid proxy can also be security weaknesses that demand extra caution and mitigation
574   considerations.



                                                      15
      NIST SP 800-223 ipd (Initial Public Draft)                        High-Performance Computing Security
      February 2023

575   Employing multiple physical networks is a common and effective means for access control and
576   fault tolerance, and it is highly recommended.

577           Compute Node Sanitization
578   High-performance compute nodes are used by multiple tenants and projects. At the end of a task
579   run, the previous project may leave behind a residual “footprint,” such as the data in memory and
580   GPUs. It is important to sanitize the compute node so that data from previous jobs are not
581   accidently leaked and the new job can start with a clean slate.
582   The common practice of compute node sanitization includes:
583       •   Conducting a node health check at the end of a job
584       •   Removing a node or forcing a reboot if a node is deemed “unhealthy”
585       •   Working with hardware and software vendors to provide management hooks to sanitize
586           the GPU
587       •   Resetting GPUs to remove residual data between jobs
588       •   Validating and checking firmware
589       •   Rebooting nodes after the completion of a job at the OS level to remove accumulated
590           residuals and ensure a consistent node state
591       •   Checking critical files to ensure that they have not been changed
592   Compute node sanitization is highly recommended as the compute nodes are equipped with
593   sophisticated hardware accelerators.

594           Data Integrity Protection
595   Data encryption is an effective means of providing data integrity. HPC data storage systems
596   typically support uniform encryption at the file level or block level. Such data encryption at the
597   file system level protects data from unauthorized access. However, it does not provide granular
598   access (i.e., segmenting one user from others). Granular access can be achieved using user-level
599   or group-level encryption. Even a system administrator cannot access a user’s data with granular
600   access. Additionally, file systems do not authorize users. Rather, users access the file system via
601   the HPC access zone, which is responsible for authenticating the users and their access rights.
602   Hashing is another technique for protecting data integrity. Data files can be hashed at the
603   beginning to acquire hashing keys. A file is not modified if its hashing key remains the same.
604   Parallel file systems maintain many types of metadata (e.g., user ID, group ID, modification
605   time, sh54 of file, etc.). Hashing metadata is also another way to check whether a file has been
606   modified.
607   Periodically scanning file systems for malware is a proven technique for ensuring data integrity.
608   However, scanning an HPC system for malware is challenging. HPC data storage can easily
609   contain a petabyte or more of data. Existing malware scanning tools are only designed for a
610   single machine or laptop using one thread. They are not efficient or fast enough to scan large



                                                      16
      NIST SP 800-223 ipd (Initial Public Draft)                        High-Performance Computing Security
      February 2023

611   HPC data storage systems. Furthermore, the scanning operation can adversely affect the
612   performance of running jobs.
613   Protecting data integrity is vital to the HPC security. Granular data access provides the best
614   protection and is highly recommended when possible.

615           Securing Containers
616   Containers bundle an application’s code, related libraries, configuration files, and required
617   dependencies to allow the applications to run seamlessly across environments. Containers
618   provide the benefits of portability, reproducibility, and productivity, but they can hide software.
619   In a well-managed HPC system, a lot of software has already been installed as a baseline system
620   environment. The applications developed using native libraries often run faster than a container.
621   Hence, training users to develop programs in the HPC programming environment is one way to
622   reduce exposure to container vulnerabilities.
623   Using containers in an HPC is a balancing act. When containers are supported, restrictions are
624   recommended to mitigate vulnerabilities. This may include prohibiting independent network
625   stacks or user namespaces (i.e., container namespace is always mapped to the host user account).
626   These measures can ensure that the risks posed by containers are no different from other
627   workloads. The threats of a container only affect the applications run by the same user. Other
628   security measures include selecting a container runtime that does not require root access to the
629   system and prohibiting container development in the HPC.

630           Achieving Security While Maintaining HPC Performance
631   HPC security measures often come with an undesirable performance penalty. The following are
632   several effective ways to balance performance and security:
633       •   Conduct tests to measure the performance penalty of security tools, which can be
634           benchmarked to determine whether they are acceptable. Testing and measurement would
635           also encourage more performance-aware tool design.
636       •   Incorporate security requirements in the initial HPC design rather than as an afterthought.
637           For instance, independent add-on security tools tend to have more impact on performance
638           than native security measures that come with the HPC software stack.
639       •   Avoid “one size fits all” security. Differentiate the types of nodes in the HPC system and
640           apply appropriate security rules and controls to different node types. For instance,
641           classify the nodes in HPC into three categories: external nodes, internal nodes, and
642           backend nodes. Apply individual security controls to each node category. Such a
643           differentiation also mitigates performance impacts.

644           Challenges to HPC Security Tools
645   Many industrial security tools are designed with stand-alone devices in mind (e.g., laptops,
646   desktops, or mobile devices). HPC is a large-scale, complex system with strict performance
647   requirements. Security tools that are effective for individual devices may not work well in an
648   HPC environment. For example, a forensic tool that aids the recovery and preservation of a hard


                                                       17
      NIST SP 800-223 ipd (Initial Public Draft)                      High-Performance Computing Security
      February 2023

649   drive and memory for a single server works well in practice. It is unreasonable, however, to
650   install forensic tools on all compute and storage nodes in an HPC system. As another example,
651   HPC nodes may use remotely mounted storage, which may disable some security tools.
652   Moreover, different HPC applications may require different tools. Security tool vendors are often
653   not accustomed to HPC use cases and requirements, which forces HPC security teams to develop
654   analogous tools that may introduce new security vulnerabilities and are sometimes not accepted
655   by organizations. The HPC community needs to work closely with security tool vendors to
656   address these challenges.
657   A Security Technical Implementation Guide (STIG) [41] is a configuration standard and offers a
658   security baseline that reflects security guidance requirements. The security checking tool can
659   measure how well the STIG is satisfied. However, available STIGs are typically written for
660   servers or desktops rather than for HPC. In addition, security baseline checking tools developed
661   for commodity operating systems and applications require customizations to run on HPC. The
662   Lawrence Livermore National Laboratory [42], Sandia National Laboratories [43], and the Los
663   Alamos National Lab [44] have collaborated with the Defense Information Systems Agency
664   (DISA) [45] to develop TOSS 4 STIG [46], which is geared toward HPC systems. Still, a more
665   general STIG library and corresponding security checking tools are desirable to handle diverse
666   subsystems and components inside an HPC.




                                                     18
      NIST SP 800-223 ipd (Initial Public Draft)                     High-Performance Computing Security
      February 2023

667       Conclusions
668   Securing HPC systems is challenging due to their size; performance requirements; diverse and
669   complex hardware, software, and applications; varying security requirements; and the nature of
670   shared resources. The security tools suitable for HPC are inadequate, and current standards and
671   guidelines on HPC security best practices are lacking. The continuous evolution of HPC systems
672   makes the task of securing them even more difficult.
673   This Special Publication aims to standardize and facilitate the information and knowledge-
674   sharing of HPC security. A zone based HPC system reference model is introduced to serve as a
675   foundation for a system lexicon and captures common features across the majority of HPC
676   systems. HPC system threat analysis is discussed, security postures and challenges are
677   considered, and recommendations are made.
678




                                                    19
      NIST SP 800-223 ipd (Initial Public Draft)                     High-Performance Computing Security
      February 2023

679   References
680
      [1]  "What Is a GPU?," [Online]. Available:
           https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html.
      [2] R. Sheldon, "InfiniBand," [Online]. Available:
           https://www.techtarget.com/searchstorage/definition/InfiniBand.
      [3] "Omni-Path," [Online]. Available: https://en.wikipedia.org/wiki/Omni-
           Path#:~:text=Omni%2DPath%20Architecture%20(OPA),this%20architecture%20for%20e
           xascale%20computing..
      [4] "HPE Slingshot interconnect," [Online]. Available:
           https://www.hpe.com/us/en/compute/hpc/slingshot-interconnect.html.
      [5] "Lustre file system," [Online]. Available: https://www.lustre.org/.
      [6] "Introducing General Parallel File System," [Online]. Available:
           https://www.ibm.com/docs/en/gpfs/4.1.0.4?topic=guide-introducing-general-parallel-file-
           system.
      [7] "IBM Spectrum Scale," [Online]. Available: https://www.ibm.com/products/spectrum-
           scale.
      [8] B. W. S. a. J. B. Lei Cao, "To share or not to share: comparing burst buffer architectures,"
           in Proceedings of the 25th High Performance Computing Symposium , Virginia Beach,
           VA, USA, April 23 - 26, 2017.
      [9] "Network File System," [Online]. Available:
           https://en.wikipedia.org/wiki/Network_File_System.
      [10] "CIFS," [Online]. Available: https://cifs.com/.
      [11] "Server Message Block Overview," [Online]. Available: https://learn.microsoft.com/en-
           us/previous-versions/windows/it-pro/windows-server-2012-r2-and-
           2012/hh831795(v=ws.11).
      [12] "FUSE," [Online]. Available:
           https://www.kernel.org/doc/html/latest/filesystems/fuse.html.
      [13] "SSHFS Homepage," [Online]. Available: https://github.com/libfuse/sshfs.
      [14] "Domain Name Services," [Online]. Available: https://www.iana.org/domains.
      [15] "DHCP," [Online]. Available:
           https://www.techtarget.com/searchnetworking/definition/DHCP.
      [16] "LDAP (Lightweight Directory Access Protocol)," [Online]. Available:
           https://www.techtarget.com/searchmobilecomputing/definition/LDAP.
      [17] "What is NTP?," [Online]. Available: http://www.ntp.org/ntpfaq/NTP-s-def.htm.
      [18] "SLURM Workload Manager," [Online]. Available:
           https://slurm.schedmd.com/documentation.html.
      [19] "Kubernetes," [Online]. Available: https://kubernetes.io/.
      [20] "HDF5," [Online]. Available: https://www.hdfgroup.org/solutions/hdf5/.
      [21] "rminer: Data Mining Classification and Regression Methods," [Online]. Available:
           https://cran.r-project.org/web/packages/rminer/index.html.



                                                    20
NIST SP 800-223 ipd (Initial Public Draft)                   High-Performance Computing Security
February 2023

[22] "What is a CI/CD pipeline?," [Online]. Available:
     https://www.redhat.com/en/topics/devops/what-cicd-
     pipeline#:~:text=A%20continuous%20integration%20and%20continuous,development%2
     0life%20cycle%20via%20automation..
[23] "CVE," [Online]. Available: https://cve.mitre.org/.
[24] "Conda," [Online]. Available: https://docs.conda.io/en/latest/.
[25] "EasyBuild: building software with ease," [Online]. Available: https://easybuild.io/.
[26] "Spack," [Online]. Available: https://spack.io/.
[27] "Package Management," [Online]. Available: https://ubuntu.com/server/docs/package-
     management#:~:text=Apt,upgrading%20the%20entire%20Ubuntu%20system..
[28] "Yum," [Online]. Available: https://en.wikipedia.org/wiki/Yum_(software).
[29] "PXE," [Online]. Available:
     https://www.techtarget.com/searchnetworking/definition/Preboot-Execution-Environment.
[30] "iSCSI (Internet Small Computer System Interface)," [Online]. Available:
     https://www.techtarget.com/searchstorage/definition/iSCSI.
[31] "Virtualizing High Performance Computing (HPC) Environments: Dell EMC Reference
     Architecture," May 2020. [Online]. Available:
     https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/solutions/VMwar
     e-HPC-Virtualized-DTC.pdf.
[32] HPCWire, "Hybrid HPC: The time to embrace the cloud is now," 2020. [Online].
     Available: https://www.hpcwire.com/2020/08/31/hybrid-hpc-the-time-to-embrace-the-
     cloud-is-now/.
[33] "What is SELinux?," [Online]. Available: https://www.redhat.com/en/topics/linux/what-is-
     selinux.
[34] "Intelligent Platform Management Interface," [Online]. Available:
     https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface.
[35] "BIOS," [Online]. Available: https://en.wikipedia.org/wiki/BIOS.
[36] "UEFI," [Online]. Available: https://uefi.org/.
[37] "MPI Forum," [Online]. Available: https://www.mpi-forum.org/.
[38] "How to share files with Samba," [Online]. Available:
     https://www.redhat.com/sysadmin/samba-file-sharing.
[39] "Network address translation," [Online]. Available:
     https://en.wikipedia.org/wiki/Network_address_translation.
[40] "Squid-cache.org," [Online]. Available: http://www.squid-cache.org/.
[41] "Security Technical Implementation Guides (STIGs)," [Online]. Available:
     https://public.cyber.mil/stigs/.
[42] "Lawrence Livermore National Laboratory," [Online]. Available: https://www.llnl.gov/.
[43] "Sandia National Laboratories," [Online]. Available: https://www.sandia.gov/.
[44] "Los Alamos National Lab," [Online]. Available: https://www.lanl.gov/.
[45] "Defense Information Systems Agency," [Online]. Available: https://disa.mil/.




                                             21
      NIST SP 800-223 ipd (Initial Public Draft)                     High-Performance Computing Security
      February 2023

      [46] "DISA releases the TOSS 4 Security Technical Implementation Guide," [Online].
           Available: https://public.cyber.mil/announcement/disa-releases-the-toss-4-security-
           technical-implementation-guide/.
681




                                                    22
